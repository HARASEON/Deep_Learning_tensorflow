{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEaIT7QSPq9N8ErDK76XL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day5_Diesel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diesel\n",
        "* Data 특징: 미국 디젤 가격 예측 1424개 데이터\n",
        "* val_loss < 0.026"
      ],
      "metadata": {
        "id": "HOmzzZJ3l1IN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5foTR1Vrl0Bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dc7628-3b55-4d59-8864-ebed57d17f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mWE6T0StX9nB",
        "outputId": "b67ca0a7-c11d-4596-a004-aaf47c5c4fc0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.10.0\n",
            "Uninstalling tensorflow-2.10.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.10.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.10.0\n",
            "  Using cached tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.2.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.1)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (16.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.53.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (67.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.17.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AObMJI3Yh-7m",
        "outputId": "9570bc37-6113-4e08-9239-29e4c5580cee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.10.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXKNzSjCJHMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3340e2-899b-4c19-8b45-0b0793407f8e"
      },
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters = 32, kernel_size = 5, padding = 'causal', activation = 'relu', input_shape = [N_PAST, 1]),\n",
        "        Bidirectional(LSTM(64, return_sequences = True)),\n",
        "        Bidirectional(LSTM(32, return_sequences = True)),\n",
        "        Dense(256, activation = 'relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(16, activation = 'relu'),\n",
        "        Dense(1),\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since the model predicts 1 feature.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 save_weights_only = True,\n",
        "                                 save_best_only = True,\n",
        "                                 monitor = 'val_mae',\n",
        "                                 verbose = 1)\n",
        "    early_stopping = EarlyStopping(monitor= 'val_mae', patience = 10)\n",
        "\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    optimizer =  tf.keras.optimizers.Adam(0.0001)\n",
        "    model.compile(optimizer = optimizer,loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
        "  \n",
        "    model.fit(train_set, validation_data = (valid_set), epochs= 100, callbacks = [checkpoint, early_stopping])\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"disel.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "# rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "# rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, 0]\n",
        "\n",
        "# x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "# result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     34/Unknown - 7s 19ms/step - loss: 0.0518 - mae: 0.2478\n",
            "Epoch 1: val_mae improved from inf to 0.30771, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 10s 101ms/step - loss: 0.0514 - mae: 0.2486 - val_loss: 0.0493 - val_mae: 0.3077\n",
            "Epoch 2/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0291 - mae: 0.1736\n",
            "Epoch 2: val_mae improved from 0.30771 to 0.18377, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0288 - mae: 0.1752 - val_loss: 0.0185 - val_mae: 0.1838\n",
            "Epoch 3/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0095 - mae: 0.1117\n",
            "Epoch 3: val_mae improved from 0.18377 to 0.04559, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0091 - mae: 0.1085 - val_loss: 0.0017 - val_mae: 0.0456\n",
            "Epoch 4/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0084 - mae: 0.1136\n",
            "Epoch 4: val_mae improved from 0.04559 to 0.04378, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.1093 - val_loss: 0.0015 - val_mae: 0.0438\n",
            "Epoch 5/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0952\n",
            "Epoch 5: val_mae improved from 0.04378 to 0.03936, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0060 - mae: 0.0941 - val_loss: 0.0012 - val_mae: 0.0394\n",
            "Epoch 6/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0045 - mae: 0.0806\n",
            "Epoch 6: val_mae improved from 0.03936 to 0.03635, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0044 - mae: 0.0797 - val_loss: 0.0011 - val_mae: 0.0364\n",
            "Epoch 7/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0676\n",
            "Epoch 7: val_mae improved from 0.03635 to 0.03480, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0034 - mae: 0.0675 - val_loss: 0.0010 - val_mae: 0.0348\n",
            "Epoch 8/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0584\n",
            "Epoch 8: val_mae improved from 0.03480 to 0.03392, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0028 - mae: 0.0584 - val_loss: 9.8941e-04 - val_mae: 0.0339\n",
            "Epoch 9/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0024 - mae: 0.0513\n",
            "Epoch 9: val_mae improved from 0.03392 to 0.03338, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0025 - mae: 0.0519 - val_loss: 9.6813e-04 - val_mae: 0.0334\n",
            "Epoch 10/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0022 - mae: 0.0469\n",
            "Epoch 10: val_mae improved from 0.03338 to 0.03272, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0023 - mae: 0.0481 - val_loss: 9.3894e-04 - val_mae: 0.0327\n",
            "Epoch 11/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0021 - mae: 0.0440\n",
            "Epoch 11: val_mae improved from 0.03272 to 0.03191, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0022 - mae: 0.0451 - val_loss: 9.0132e-04 - val_mae: 0.0319\n",
            "Epoch 12/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0022 - mae: 0.0441\n",
            "Epoch 12: val_mae improved from 0.03191 to 0.03128, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 27ms/step - loss: 0.0021 - mae: 0.0440 - val_loss: 8.7261e-04 - val_mae: 0.0313\n",
            "Epoch 13/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0021 - mae: 0.0428\n",
            "Epoch 13: val_mae improved from 0.03128 to 0.03019, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 29ms/step - loss: 0.0021 - mae: 0.0428 - val_loss: 8.2701e-04 - val_mae: 0.0302\n",
            "Epoch 14/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0423\n",
            "Epoch 14: val_mae improved from 0.03019 to 0.02925, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 29ms/step - loss: 0.0020 - mae: 0.0423 - val_loss: 7.8905e-04 - val_mae: 0.0292\n",
            "Epoch 15/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0403\n",
            "Epoch 15: val_mae improved from 0.02925 to 0.02861, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.0019 - mae: 0.0415 - val_loss: 7.6571e-04 - val_mae: 0.0286\n",
            "Epoch 16/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0414\n",
            "Epoch 16: val_mae improved from 0.02861 to 0.02831, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0020 - mae: 0.0414 - val_loss: 7.5006e-04 - val_mae: 0.0283\n",
            "Epoch 17/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0392\n",
            "Epoch 17: val_mae improved from 0.02831 to 0.02775, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0019 - mae: 0.0409 - val_loss: 7.2804e-04 - val_mae: 0.0278\n",
            "Epoch 18/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0018 - mae: 0.0390\n",
            "Epoch 18: val_mae improved from 0.02775 to 0.02749, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0019 - mae: 0.0404 - val_loss: 7.1616e-04 - val_mae: 0.0275\n",
            "Epoch 19/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0018 - mae: 0.0379\n",
            "Epoch 19: val_mae improved from 0.02749 to 0.02735, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0019 - mae: 0.0396 - val_loss: 7.0828e-04 - val_mae: 0.0273\n",
            "Epoch 20/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0392\n",
            "Epoch 20: val_mae improved from 0.02735 to 0.02710, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0019 - mae: 0.0394 - val_loss: 6.9666e-04 - val_mae: 0.0271\n",
            "Epoch 21/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0018 - mae: 0.0381\n",
            "Epoch 21: val_mae improved from 0.02710 to 0.02690, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0019 - mae: 0.0395 - val_loss: 6.8778e-04 - val_mae: 0.0269\n",
            "Epoch 22/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0376\n",
            "Epoch 22: val_mae improved from 0.02690 to 0.02671, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0018 - mae: 0.0390 - val_loss: 6.7852e-04 - val_mae: 0.0267\n",
            "Epoch 23/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0380\n",
            "Epoch 23: val_mae improved from 0.02671 to 0.02659, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0017 - mae: 0.0382 - val_loss: 6.7105e-04 - val_mae: 0.0266\n",
            "Epoch 24/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0381\n",
            "Epoch 24: val_mae improved from 0.02659 to 0.02651, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0018 - mae: 0.0384 - val_loss: 6.6657e-04 - val_mae: 0.0265\n",
            "Epoch 25/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0017 - mae: 0.0364\n",
            "Epoch 25: val_mae improved from 0.02651 to 0.02637, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0017 - mae: 0.0381 - val_loss: 6.6033e-04 - val_mae: 0.0264\n",
            "Epoch 26/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0017 - mae: 0.0368\n",
            "Epoch 26: val_mae improved from 0.02637 to 0.02634, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0018 - mae: 0.0385 - val_loss: 6.5746e-04 - val_mae: 0.0263\n",
            "Epoch 27/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0368\n",
            "Epoch 27: val_mae improved from 0.02634 to 0.02632, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 29ms/step - loss: 0.0018 - mae: 0.0382 - val_loss: 6.5497e-04 - val_mae: 0.0263\n",
            "Epoch 28/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0017 - mae: 0.0362\n",
            "Epoch 28: val_mae improved from 0.02632 to 0.02627, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.0017 - mae: 0.0379 - val_loss: 6.5136e-04 - val_mae: 0.0263\n",
            "Epoch 29/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0363\n",
            "Epoch 29: val_mae improved from 0.02627 to 0.02619, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.0017 - mae: 0.0378 - val_loss: 6.4680e-04 - val_mae: 0.0262\n",
            "Epoch 30/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0360\n",
            "Epoch 30: val_mae improved from 0.02619 to 0.02614, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0017 - mae: 0.0376 - val_loss: 6.4354e-04 - val_mae: 0.0261\n",
            "Epoch 31/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0357\n",
            "Epoch 31: val_mae did not improve from 0.02614\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0017 - mae: 0.0372 - val_loss: 6.4605e-04 - val_mae: 0.0263\n",
            "Epoch 32/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0371\n",
            "Epoch 32: val_mae improved from 0.02614 to 0.02590, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0017 - mae: 0.0373 - val_loss: 6.3180e-04 - val_mae: 0.0259\n",
            "Epoch 33/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0367\n",
            "Epoch 33: val_mae improved from 0.02590 to 0.02586, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0367 - val_loss: 6.2910e-04 - val_mae: 0.0259\n",
            "Epoch 34/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0366\n",
            "Epoch 34: val_mae did not improve from 0.02586\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0016 - mae: 0.0370 - val_loss: 6.5354e-04 - val_mae: 0.0267\n",
            "Epoch 35/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0368\n",
            "Epoch 35: val_mae improved from 0.02586 to 0.02584, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0371 - val_loss: 6.2614e-04 - val_mae: 0.0258\n",
            "Epoch 36/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0369\n",
            "Epoch 36: val_mae did not improve from 0.02584\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0369 - val_loss: 6.4141e-04 - val_mae: 0.0263\n",
            "Epoch 37/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0361\n",
            "Epoch 37: val_mae improved from 0.02584 to 0.02567, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0364 - val_loss: 6.1702e-04 - val_mae: 0.0257\n",
            "Epoch 38/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0015 - mae: 0.0347\n",
            "Epoch 38: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0016 - mae: 0.0363 - val_loss: 6.3684e-04 - val_mae: 0.0263\n",
            "Epoch 39/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0015 - mae: 0.0348\n",
            "Epoch 39: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0016 - mae: 0.0367 - val_loss: 6.3377e-04 - val_mae: 0.0262\n",
            "Epoch 40/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0015 - mae: 0.0345\n",
            "Epoch 40: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0365 - val_loss: 6.3092e-04 - val_mae: 0.0262\n",
            "Epoch 41/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0015 - mae: 0.0347\n",
            "Epoch 41: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0364 - val_loss: 6.2364e-04 - val_mae: 0.0260\n",
            "Epoch 42/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0015 - mae: 0.0345\n",
            "Epoch 42: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0364 - val_loss: 6.4160e-04 - val_mae: 0.0266\n",
            "Epoch 43/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0014 - mae: 0.0339\n",
            "Epoch 43: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0359 - val_loss: 6.1285e-04 - val_mae: 0.0258\n",
            "Epoch 44/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0361\n",
            "Epoch 44: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0016 - mae: 0.0366 - val_loss: 6.4250e-04 - val_mae: 0.0267\n",
            "Epoch 45/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0014 - mae: 0.0342\n",
            "Epoch 45: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 27ms/step - loss: 0.0015 - mae: 0.0359 - val_loss: 6.1948e-04 - val_mae: 0.0261\n",
            "Epoch 46/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0015 - mae: 0.0356\n",
            "Epoch 46: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 27ms/step - loss: 0.0015 - mae: 0.0356 - val_loss: 6.1924e-04 - val_mae: 0.0261\n",
            "Epoch 47/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0015 - mae: 0.0353\n",
            "Epoch 47: val_mae did not improve from 0.02567\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.0016 - mae: 0.0359 - val_loss: 6.0931e-04 - val_mae: 0.0259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "* val_loss <0.026 success! 0.021\n",
        "\n",
        "* 시도 : csv파일을 전부 ctrl +A로 복붙해서 명령창에 넣은 다음, 이 내용을 csv파일로 변환하여 RNN과 CNN을 모두 사용하는 tensorflow를 이용한 코드를 짜라 조건은 val_loss가 0.02 보다 작아야하고 에러가 생기면 안된다.\n",
        "* 결과: 대답이 나오지 않고 에러가 났다.\n",
        "* 아이디어 : 굳이 데이터를 넣지 않고 코드의 퍼포먼스만 향상시키도록 명령을 내린다.\n",
        "* 배운점: optimizer =  tf.keras.optimizers.Adam(0.00001) <- 여기 숫자를 잘고치면 val_loss가 낮게 진행속도는 더 느려진다.\n",
        "* prompt: modify this code more efficient and precise to get lower val_loss under 0.02. Do not make any error.\n",
        "* responds:\n",
        "This model adds additional convolutional layers with more filters and larger LSTM layers. It also adds dropout layers after each convolutional and dense layer to help prevent overfitting. You may need to experiment with the exact architecture and hyperparameters to find the best results for your specific problem."
      ],
      "metadata": {
        "id": "3wMPFKLPXfHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, BatchNormalization, Dropout,MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        # Dropout(0.3),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1),\n",
        "\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since the model predicts 1 feature.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 save_weights_only = True,\n",
        "                                 save_best_only = True,\n",
        "                                 monitor = 'val_mae',\n",
        "                                 verbose = 1)\n",
        "    early_stopping = EarlyStopping(monitor= 'val_mae', patience = 10)\n",
        "\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    optimizer =  tf.keras.optimizers.Adam(0.0001)\n",
        "    model.compile(optimizer = optimizer,loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
        "  \n",
        "    model.fit(train_set, validation_data = (valid_set), epochs= 100, callbacks = [checkpoint, early_stopping])\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"diesel_gpt.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "# rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "# rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, 0]\n",
        "\n",
        "# x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "# result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuGUK283cckN",
        "outputId": "2e3b2f12-51f3-4eef-e15d-d857c29fbc43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     31/Unknown - 6s 12ms/step - loss: 0.0249 - mae: 0.1664\n",
            "Epoch 1: val_mae improved from inf to 0.13430, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 8s 61ms/step - loss: 0.0258 - mae: 0.1731 - val_loss: 0.0101 - val_mae: 0.1343\n",
            "Epoch 2/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0077 - mae: 0.0999\n",
            "Epoch 2: val_mae improved from 0.13430 to 0.03084, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.1000 - val_loss: 7.4056e-04 - val_mae: 0.0308\n",
            "Epoch 3/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.1056\n",
            "Epoch 3: val_mae improved from 0.03084 to 0.02894, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.1050 - val_loss: 7.0534e-04 - val_mae: 0.0289\n",
            "Epoch 4/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0058 - mae: 0.0853\n",
            "Epoch 4: val_mae did not improve from 0.02894\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0058 - mae: 0.0852 - val_loss: 7.7674e-04 - val_mae: 0.0299\n",
            "Epoch 5/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0760\n",
            "Epoch 5: val_mae improved from 0.02894 to 0.02872, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0053 - mae: 0.0760 - val_loss: 7.4984e-04 - val_mae: 0.0287\n",
            "Epoch 6/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0042 - mae: 0.0659\n",
            "Epoch 6: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0042 - mae: 0.0661 - val_loss: 8.7054e-04 - val_mae: 0.0317\n",
            "Epoch 7/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0045 - mae: 0.0631\n",
            "Epoch 7: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0045 - mae: 0.0632 - val_loss: 8.7871e-04 - val_mae: 0.0319\n",
            "Epoch 8/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0041 - mae: 0.0578\n",
            "Epoch 8: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 27ms/step - loss: 0.0044 - mae: 0.0612 - val_loss: 8.1101e-04 - val_mae: 0.0300\n",
            "Epoch 9/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0046 - mae: 0.0630\n",
            "Epoch 9: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0046 - mae: 0.0633 - val_loss: 0.0011 - val_mae: 0.0376\n",
            "Epoch 10/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0043 - mae: 0.0604\n",
            "Epoch 10: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0043 - mae: 0.0611 - val_loss: 7.9354e-04 - val_mae: 0.0297\n",
            "Epoch 11/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0042 - mae: 0.0601\n",
            "Epoch 11: val_mae did not improve from 0.02872\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0042 - mae: 0.0601 - val_loss: 8.7023e-04 - val_mae: 0.0319\n",
            "Epoch 12/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0041 - mae: 0.0591\n",
            "Epoch 12: val_mae improved from 0.02872 to 0.02775, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0593 - val_loss: 7.2709e-04 - val_mae: 0.0277\n",
            "Epoch 13/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0040 - mae: 0.0598\n",
            "Epoch 13: val_mae did not improve from 0.02775\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0040 - mae: 0.0598 - val_loss: 0.0010 - val_mae: 0.0360\n",
            "Epoch 14/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0037 - mae: 0.0551\n",
            "Epoch 14: val_mae improved from 0.02775 to 0.02749, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0588 - val_loss: 7.1610e-04 - val_mae: 0.0275\n",
            "Epoch 15/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0037 - mae: 0.0570\n",
            "Epoch 15: val_mae did not improve from 0.02749\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0038 - mae: 0.0591 - val_loss: 7.1737e-04 - val_mae: 0.0276\n",
            "Epoch 16/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0035 - mae: 0.0562\n",
            "Epoch 16: val_mae did not improve from 0.02749\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0035 - mae: 0.0562 - val_loss: 7.4928e-04 - val_mae: 0.0285\n",
            "Epoch 17/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0033 - mae: 0.0513\n",
            "Epoch 17: val_mae improved from 0.02749 to 0.02729, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0037 - mae: 0.0556 - val_loss: 7.0278e-04 - val_mae: 0.0273\n",
            "Epoch 18/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0030 - mae: 0.0516\n",
            "Epoch 18: val_mae did not improve from 0.02729\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0033 - mae: 0.0548 - val_loss: 7.2298e-04 - val_mae: 0.0277\n",
            "Epoch 19/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0033 - mae: 0.0546\n",
            "Epoch 19: val_mae did not improve from 0.02729\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0033 - mae: 0.0550 - val_loss: 7.2564e-04 - val_mae: 0.0281\n",
            "Epoch 20/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0533\n",
            "Epoch 20: val_mae did not improve from 0.02729\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0033 - mae: 0.0533 - val_loss: 7.1144e-04 - val_mae: 0.0279\n",
            "Epoch 21/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0035 - mae: 0.0552\n",
            "Epoch 21: val_mae improved from 0.02729 to 0.02715, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0035 - mae: 0.0552 - val_loss: 6.8489e-04 - val_mae: 0.0271\n",
            "Epoch 22/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0032 - mae: 0.0517\n",
            "Epoch 22: val_mae did not improve from 0.02715\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0035 - mae: 0.0549 - val_loss: 6.9088e-04 - val_mae: 0.0275\n",
            "Epoch 23/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0031 - mae: 0.0516\n",
            "Epoch 23: val_mae improved from 0.02715 to 0.02707, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0033 - mae: 0.0530 - val_loss: 6.9089e-04 - val_mae: 0.0271\n",
            "Epoch 24/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0034 - mae: 0.0548\n",
            "Epoch 24: val_mae did not improve from 0.02707\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0034 - mae: 0.0548 - val_loss: 7.2982e-04 - val_mae: 0.0285\n",
            "Epoch 25/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0033 - mae: 0.0500\n",
            "Epoch 25: val_mae did not improve from 0.02707\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0035 - mae: 0.0536 - val_loss: 6.7919e-04 - val_mae: 0.0273\n",
            "Epoch 26/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0028 - mae: 0.0475\n",
            "Epoch 26: val_mae did not improve from 0.02707\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0030 - mae: 0.0509 - val_loss: 7.1004e-04 - val_mae: 0.0278\n",
            "Epoch 27/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0035 - mae: 0.0532\n",
            "Epoch 27: val_mae improved from 0.02707 to 0.02686, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0036 - mae: 0.0550 - val_loss: 6.7643e-04 - val_mae: 0.0269\n",
            "Epoch 28/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0032 - mae: 0.0525\n",
            "Epoch 28: val_mae did not improve from 0.02686\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0032 - mae: 0.0531 - val_loss: 6.6976e-04 - val_mae: 0.0269\n",
            "Epoch 29/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0036 - mae: 0.0536\n",
            "Epoch 29: val_mae improved from 0.02686 to 0.02686, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 29ms/step - loss: 0.0038 - mae: 0.0556 - val_loss: 6.6995e-04 - val_mae: 0.0269\n",
            "Epoch 30/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0033 - mae: 0.0517\n",
            "Epoch 30: val_mae did not improve from 0.02686\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0035 - mae: 0.0543 - val_loss: 7.2067e-04 - val_mae: 0.0282\n",
            "Epoch 31/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0035 - mae: 0.0537\n",
            "Epoch 31: val_mae improved from 0.02686 to 0.02675, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0036 - mae: 0.0557 - val_loss: 6.6871e-04 - val_mae: 0.0268\n",
            "Epoch 32/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0521\n",
            "Epoch 32: val_mae improved from 0.02675 to 0.02672, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0034 - mae: 0.0526 - val_loss: 6.6534e-04 - val_mae: 0.0267\n",
            "Epoch 33/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0033 - mae: 0.0509\n",
            "Epoch 33: val_mae improved from 0.02672 to 0.02670, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0034 - mae: 0.0531 - val_loss: 6.6797e-04 - val_mae: 0.0267\n",
            "Epoch 34/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0030 - mae: 0.0482\n",
            "Epoch 34: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0032 - mae: 0.0511 - val_loss: 6.7055e-04 - val_mae: 0.0271\n",
            "Epoch 35/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0032 - mae: 0.0521\n",
            "Epoch 35: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0032 - mae: 0.0521 - val_loss: 6.5621e-04 - val_mae: 0.0267\n",
            "Epoch 36/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0028 - mae: 0.0483\n",
            "Epoch 36: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0031 - mae: 0.0522 - val_loss: 7.2188e-04 - val_mae: 0.0283\n",
            "Epoch 37/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0028 - mae: 0.0478\n",
            "Epoch 37: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0031 - mae: 0.0516 - val_loss: 6.6060e-04 - val_mae: 0.0270\n",
            "Epoch 38/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0033 - mae: 0.0518\n",
            "Epoch 38: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0033 - mae: 0.0518 - val_loss: 6.8157e-04 - val_mae: 0.0273\n",
            "Epoch 39/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0030 - mae: 0.0491\n",
            "Epoch 39: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0031 - mae: 0.0509 - val_loss: 7.6819e-04 - val_mae: 0.0296\n",
            "Epoch 40/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0535\n",
            "Epoch 40: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0034 - mae: 0.0539 - val_loss: 6.7592e-04 - val_mae: 0.0274\n",
            "Epoch 41/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0490\n",
            "Epoch 41: val_mae did not improve from 0.02670\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0029 - mae: 0.0490 - val_loss: 6.9210e-04 - val_mae: 0.0276\n",
            "Epoch 42/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0036 - mae: 0.0528\n",
            "Epoch 42: val_mae improved from 0.02670 to 0.02642, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0036 - mae: 0.0532 - val_loss: 6.4842e-04 - val_mae: 0.0264\n",
            "Epoch 43/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0027 - mae: 0.0471\n",
            "Epoch 43: val_mae improved from 0.02642 to 0.02640, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0028 - mae: 0.0487 - val_loss: 6.4569e-04 - val_mae: 0.0264\n",
            "Epoch 44/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0028 - mae: 0.0481\n",
            "Epoch 44: val_mae did not improve from 0.02640\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0509 - val_loss: 6.4830e-04 - val_mae: 0.0264\n",
            "Epoch 45/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0028 - mae: 0.0459\n",
            "Epoch 45: val_mae did not improve from 0.02640\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0032 - mae: 0.0506 - val_loss: 6.4479e-04 - val_mae: 0.0264\n",
            "Epoch 46/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0028 - mae: 0.0480\n",
            "Epoch 46: val_mae did not improve from 0.02640\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0512 - val_loss: 6.4366e-04 - val_mae: 0.0264\n",
            "Epoch 47/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0028 - mae: 0.0476\n",
            "Epoch 47: val_mae improved from 0.02640 to 0.02621, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0029 - mae: 0.0495 - val_loss: 6.3008e-04 - val_mae: 0.0262\n",
            "Epoch 48/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0030 - mae: 0.0488\n",
            "Epoch 48: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0031 - mae: 0.0511 - val_loss: 6.5317e-04 - val_mae: 0.0267\n",
            "Epoch 49/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0027 - mae: 0.0466\n",
            "Epoch 49: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0029 - mae: 0.0494 - val_loss: 6.5029e-04 - val_mae: 0.0267\n",
            "Epoch 50/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0031 - mae: 0.0512\n",
            "Epoch 50: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0512 - val_loss: 7.1151e-04 - val_mae: 0.0283\n",
            "Epoch 51/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0490\n",
            "Epoch 51: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0029 - mae: 0.0490 - val_loss: 6.8204e-04 - val_mae: 0.0275\n",
            "Epoch 52/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0030 - mae: 0.0498\n",
            "Epoch 52: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0030 - mae: 0.0498 - val_loss: 6.5680e-04 - val_mae: 0.0268\n",
            "Epoch 53/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0456\n",
            "Epoch 53: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0028 - mae: 0.0483 - val_loss: 7.0054e-04 - val_mae: 0.0280\n",
            "Epoch 54/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0030 - mae: 0.0497\n",
            "Epoch 54: val_mae did not improve from 0.02621\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0030 - mae: 0.0497 - val_loss: 6.2702e-04 - val_mae: 0.0264\n",
            "Epoch 55/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0029 - mae: 0.0494\n",
            "Epoch 55: val_mae improved from 0.02621 to 0.02593, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0029 - mae: 0.0493 - val_loss: 6.1092e-04 - val_mae: 0.0259\n",
            "Epoch 56/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0025 - mae: 0.0461\n",
            "Epoch 56: val_mae did not improve from 0.02593\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0485 - val_loss: 7.0933e-04 - val_mae: 0.0285\n",
            "Epoch 57/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0484\n",
            "Epoch 57: val_mae did not improve from 0.02593\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0484 - val_loss: 6.6317e-04 - val_mae: 0.0270\n",
            "Epoch 58/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0029 - mae: 0.0480\n",
            "Epoch 58: val_mae did not improve from 0.02593\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0032 - mae: 0.0522 - val_loss: 6.3233e-04 - val_mae: 0.0262\n",
            "Epoch 59/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0029 - mae: 0.0481\n",
            "Epoch 59: val_mae improved from 0.02593 to 0.02579, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0031 - mae: 0.0503 - val_loss: 6.1483e-04 - val_mae: 0.0258\n",
            "Epoch 60/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0031 - mae: 0.0500\n",
            "Epoch 60: val_mae improved from 0.02579 to 0.02572, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0031 - mae: 0.0505 - val_loss: 6.0023e-04 - val_mae: 0.0257\n",
            "Epoch 61/100\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0027 - mae: 0.0458\n",
            "Epoch 61: val_mae did not improve from 0.02572\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0502 - val_loss: 7.0539e-04 - val_mae: 0.0284\n",
            "Epoch 62/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0031 - mae: 0.0506\n",
            "Epoch 62: val_mae did not improve from 0.02572\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0032 - mae: 0.0529 - val_loss: 7.9143e-04 - val_mae: 0.0306\n",
            "Epoch 63/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0031 - mae: 0.0514\n",
            "Epoch 63: val_mae did not improve from 0.02572\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0031 - mae: 0.0518 - val_loss: 6.3539e-04 - val_mae: 0.0264\n",
            "Epoch 64/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0028 - mae: 0.0486\n",
            "Epoch 64: val_mae did not improve from 0.02572\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0507 - val_loss: 7.9615e-04 - val_mae: 0.0307\n",
            "Epoch 65/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0030 - mae: 0.0496\n",
            "Epoch 65: val_mae improved from 0.02572 to 0.02571, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0031 - mae: 0.0514 - val_loss: 6.1287e-04 - val_mae: 0.0257\n",
            "Epoch 66/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0479\n",
            "Epoch 66: val_mae improved from 0.02571 to 0.02550, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0027 - mae: 0.0479 - val_loss: 5.9112e-04 - val_mae: 0.0255\n",
            "Epoch 67/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0033 - mae: 0.0519\n",
            "Epoch 67: val_mae did not improve from 0.02550\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0033 - mae: 0.0519 - val_loss: 6.8137e-04 - val_mae: 0.0279\n",
            "Epoch 68/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0502\n",
            "Epoch 68: val_mae did not improve from 0.02550\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0028 - mae: 0.0512 - val_loss: 7.0180e-04 - val_mae: 0.0284\n",
            "Epoch 69/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0479\n",
            "Epoch 69: val_mae did not improve from 0.02550\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0027 - mae: 0.0479 - val_loss: 7.0786e-04 - val_mae: 0.0285\n",
            "Epoch 70/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0480\n",
            "Epoch 70: val_mae improved from 0.02550 to 0.02521, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0028 - mae: 0.0482 - val_loss: 5.8613e-04 - val_mae: 0.0252\n",
            "Epoch 71/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0027 - mae: 0.0466\n",
            "Epoch 71: val_mae did not improve from 0.02521\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0028 - mae: 0.0494 - val_loss: 5.7826e-04 - val_mae: 0.0253\n",
            "Epoch 72/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0500\n",
            "Epoch 72: val_mae did not improve from 0.02521\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0029 - mae: 0.0500 - val_loss: 7.3939e-04 - val_mae: 0.0295\n",
            "Epoch 73/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0493\n",
            "Epoch 73: val_mae improved from 0.02521 to 0.02508, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0028 - mae: 0.0493 - val_loss: 5.7856e-04 - val_mae: 0.0251\n",
            "Epoch 74/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0457\n",
            "Epoch 74: val_mae did not improve from 0.02508\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 6.6961e-04 - val_mae: 0.0276\n",
            "Epoch 75/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0031 - mae: 0.0510\n",
            "Epoch 75: val_mae did not improve from 0.02508\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0031 - mae: 0.0510 - val_loss: 7.6736e-04 - val_mae: 0.0302\n",
            "Epoch 76/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0029 - mae: 0.0494\n",
            "Epoch 76: val_mae improved from 0.02508 to 0.02500, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0029 - mae: 0.0501 - val_loss: 5.8262e-04 - val_mae: 0.0250\n",
            "Epoch 77/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0027 - mae: 0.0479\n",
            "Epoch 77: val_mae did not improve from 0.02500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0028 - mae: 0.0492 - val_loss: 5.9261e-04 - val_mae: 0.0256\n",
            "Epoch 78/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0025 - mae: 0.0465\n",
            "Epoch 78: val_mae did not improve from 0.02500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0028 - mae: 0.0497 - val_loss: 8.2099e-04 - val_mae: 0.0316\n",
            "Epoch 79/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0474\n",
            "Epoch 79: val_mae did not improve from 0.02500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0487 - val_loss: 6.4379e-04 - val_mae: 0.0269\n",
            "Epoch 80/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0027 - mae: 0.0467\n",
            "Epoch 80: val_mae did not improve from 0.02500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0498 - val_loss: 6.1565e-04 - val_mae: 0.0261\n",
            "Epoch 81/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0477\n",
            "Epoch 81: val_mae improved from 0.02500 to 0.02489, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0025 - mae: 0.0477 - val_loss: 5.6161e-04 - val_mae: 0.0249\n",
            "Epoch 82/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0025 - mae: 0.0460\n",
            "Epoch 82: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0488 - val_loss: 7.5870e-04 - val_mae: 0.0301\n",
            "Epoch 83/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0491\n",
            "Epoch 83: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0027 - mae: 0.0491 - val_loss: 5.8389e-04 - val_mae: 0.0254\n",
            "Epoch 84/100\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0028 - mae: 0.0466\n",
            "Epoch 84: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0492 - val_loss: 6.6212e-04 - val_mae: 0.0276\n",
            "Epoch 85/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0468\n",
            "Epoch 85: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0025 - mae: 0.0472 - val_loss: 6.1160e-04 - val_mae: 0.0260\n",
            "Epoch 86/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0025 - mae: 0.0453\n",
            "Epoch 86: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0026 - mae: 0.0468 - val_loss: 5.8568e-04 - val_mae: 0.0254\n",
            "Epoch 87/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0029 - mae: 0.0498\n",
            "Epoch 87: val_mae did not improve from 0.02489\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0509 - val_loss: 6.0726e-04 - val_mae: 0.0260\n",
            "Epoch 88/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0029 - mae: 0.0489\n",
            "Epoch 88: val_mae improved from 0.02489 to 0.02438, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0030 - mae: 0.0507 - val_loss: 5.4731e-04 - val_mae: 0.0244\n",
            "Epoch 89/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0027 - mae: 0.0481\n",
            "Epoch 89: val_mae did not improve from 0.02438\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0027 - mae: 0.0487 - val_loss: 7.1153e-04 - val_mae: 0.0289\n",
            "Epoch 90/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0023 - mae: 0.0451\n",
            "Epoch 90: val_mae improved from 0.02438 to 0.02436, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0024 - mae: 0.0470 - val_loss: 5.4780e-04 - val_mae: 0.0244\n",
            "Epoch 91/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0497\n",
            "Epoch 91: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0028 - mae: 0.0497 - val_loss: 7.0007e-04 - val_mae: 0.0287\n",
            "Epoch 92/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0510\n",
            "Epoch 92: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0029 - mae: 0.0510 - val_loss: 6.4687e-04 - val_mae: 0.0271\n",
            "Epoch 93/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0473\n",
            "Epoch 93: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0027 - mae: 0.0489 - val_loss: 6.1509e-04 - val_mae: 0.0264\n",
            "Epoch 94/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0025 - mae: 0.0461\n",
            "Epoch 94: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0025 - mae: 0.0473 - val_loss: 5.4163e-04 - val_mae: 0.0245\n",
            "Epoch 95/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0486\n",
            "Epoch 95: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0028 - mae: 0.0491 - val_loss: 6.3259e-04 - val_mae: 0.0269\n",
            "Epoch 96/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0499\n",
            "Epoch 96: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0028 - mae: 0.0504 - val_loss: 6.3698e-04 - val_mae: 0.0270\n",
            "Epoch 97/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0471\n",
            "Epoch 97: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 5.8745e-04 - val_mae: 0.0257\n",
            "Epoch 98/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0025 - mae: 0.0475\n",
            "Epoch 98: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0493 - val_loss: 6.6496e-04 - val_mae: 0.0279\n",
            "Epoch 99/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0029 - mae: 0.0500\n",
            "Epoch 99: val_mae did not improve from 0.02436\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0028 - mae: 0.0502 - val_loss: 5.4722e-04 - val_mae: 0.0245\n",
            "Epoch 100/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0483\n",
            "Epoch 100: val_mae improved from 0.02436 to 0.02396, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0027 - mae: 0.0483 - val_loss: 5.2977e-04 - val_mae: 0.0240\n"
          ]
        }
      ]
    }
  ]
}