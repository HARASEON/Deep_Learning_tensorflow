{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMadM3Axz6vyP73Dso4VjqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day2_Deep_Learning_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10\n",
        "* 데이터셋 특징: 32*32 픽셀의 60000개 컬러이미지\n",
        "* 각이미지는 10개의 클래스로 라벨링 (마지막 dense layer 출력값 :10)\n",
        "* 50000 : training, 10000 : test\n",
        "* 목표: val_loss <0.5\n",
        "* CNN 사용을 권장"
      ],
      "metadata": {
        "id": "iocfLPUlnTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 with no CNN\n",
        "* loss function: CCEE(Categorical Cross Entropy Error)\n",
        "* 클래스가 3개 이상인 데이터를 대상으로 사용하는 손실함수\n",
        "* activation function: softmax\n",
        "* crossentropy를 N개의 데이터 셋에 대해 1개의 스칼라를 추출하는 방법법\n"
      ],
      "metadata": {
        "id": "Va-94CrasH43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the optimizer, loss function and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, save_weights_only=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1,\n",
        "                    validation_data=(x_test, y_test), callbacks=[checkpoint, early_stop, reduce_lr])\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the optimizer, loss function and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, save_weights_only=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1,\n",
        "                    validation_data=(x_test, y_test), callbacks=[checkpoint, early_stop, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M3BInrSam7A",
        "outputId": "090eddd9-b364-486f-85d8-021bda0bf8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 2.1308 - accuracy: 0.2328 - val_loss: 1.9492 - val_accuracy: 0.2801 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9947 - accuracy: 0.2634 - val_loss: 1.8903 - val_accuracy: 0.2979 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9572 - accuracy: 0.2785 - val_loss: 1.9116 - val_accuracy: 0.3089 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9380 - accuracy: 0.2884 - val_loss: 1.8554 - val_accuracy: 0.3349 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9075 - accuracy: 0.3017 - val_loss: 1.8278 - val_accuracy: 0.3402 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8966 - accuracy: 0.3101 - val_loss: 1.8457 - val_accuracy: 0.3448 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8830 - accuracy: 0.3184 - val_loss: 1.7904 - val_accuracy: 0.3717 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8726 - accuracy: 0.3211 - val_loss: 1.8601 - val_accuracy: 0.3303 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8794 - accuracy: 0.3171 - val_loss: 1.8283 - val_accuracy: 0.3573 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8673 - accuracy: 0.3210 - val_loss: 1.8017 - val_accuracy: 0.3717 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8692 - accuracy: 0.3167 - val_loss: 1.8534 - val_accuracy: 0.3376 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.8706 - accuracy: 0.3179\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8706 - accuracy: 0.3179 - val_loss: 1.8208 - val_accuracy: 0.3370 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8525 - accuracy: 0.3263 - val_loss: 1.8119 - val_accuracy: 0.3677 - lr: 2.0000e-04\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8417 - accuracy: 0.3276 - val_loss: 1.7969 - val_accuracy: 0.3678 - lr: 2.0000e-04\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8377 - accuracy: 0.3316 - val_loss: 1.8073 - val_accuracy: 0.3714 - lr: 2.0000e-04\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8371 - accuracy: 0.3295 - val_loss: 1.8187 - val_accuracy: 0.3714 - lr: 2.0000e-04\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8359 - accuracy: 0.3326 - val_loss: 1.7804 - val_accuracy: 0.3808 - lr: 2.0000e-04\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8350 - accuracy: 0.3316 - val_loss: 1.7802 - val_accuracy: 0.3789 - lr: 2.0000e-04\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8321 - accuracy: 0.3319 - val_loss: 1.8033 - val_accuracy: 0.3757 - lr: 2.0000e-04\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8285 - accuracy: 0.3349 - val_loss: 1.7996 - val_accuracy: 0.3637 - lr: 2.0000e-04\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8276 - accuracy: 0.3350 - val_loss: 1.8088 - val_accuracy: 0.3609 - lr: 2.0000e-04\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8295 - accuracy: 0.3364 - val_loss: 1.8271 - val_accuracy: 0.3638 - lr: 2.0000e-04\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8269 - accuracy: 0.3343 - val_loss: 1.7778 - val_accuracy: 0.3786 - lr: 2.0000e-04\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8245 - accuracy: 0.3362 - val_loss: 1.7746 - val_accuracy: 0.3829 - lr: 2.0000e-04\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8257 - accuracy: 0.3369 - val_loss: 1.7977 - val_accuracy: 0.3700 - lr: 2.0000e-04\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8258 - accuracy: 0.3346 - val_loss: 1.8053 - val_accuracy: 0.3684 - lr: 2.0000e-04\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8220 - accuracy: 0.3355 - val_loss: 1.8171 - val_accuracy: 0.3614 - lr: 2.0000e-04\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8216 - accuracy: 0.3396 - val_loss: 1.7828 - val_accuracy: 0.3726 - lr: 2.0000e-04\n",
            "Epoch 29/100\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 1.8146 - accuracy: 0.3394\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8147 - accuracy: 0.3392 - val_loss: 1.7871 - val_accuracy: 0.3680 - lr: 2.0000e-04\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8198 - accuracy: 0.3351 - val_loss: 1.7921 - val_accuracy: 0.3719 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8142 - accuracy: 0.3396 - val_loss: 1.7737 - val_accuracy: 0.3804 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8184 - accuracy: 0.3411 - val_loss: 1.7872 - val_accuracy: 0.3746 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8199 - accuracy: 0.3378 - val_loss: 1.7891 - val_accuracy: 0.3759 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8171 - accuracy: 0.3410 - val_loss: 1.7662 - val_accuracy: 0.3857 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8185 - accuracy: 0.3420 - val_loss: 1.7843 - val_accuracy: 0.3781 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8141 - accuracy: 0.3400 - val_loss: 1.7818 - val_accuracy: 0.3797 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8146 - accuracy: 0.3391 - val_loss: 1.8026 - val_accuracy: 0.3736 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8109 - accuracy: 0.3390 - val_loss: 1.7909 - val_accuracy: 0.3730 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8138 - accuracy: 0.3412 - val_loss: 1.7971 - val_accuracy: 0.3725 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8138 - accuracy: 0.3404 - val_loss: 1.7807 - val_accuracy: 0.3797 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8113 - accuracy: 0.3421 - val_loss: 1.7968 - val_accuracy: 0.3742 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8122 - accuracy: 0.3419 - val_loss: 1.8060 - val_accuracy: 0.3710 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8107 - accuracy: 0.3404 - val_loss: 1.7848 - val_accuracy: 0.3788 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8111 - accuracy: 0.3418 - val_loss: 1.7975 - val_accuracy: 0.3666 - lr: 1.0000e-04\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 11s 5ms/step - loss: 2.0998 - accuracy: 0.2370 - val_loss: 1.8670 - val_accuracy: 0.3182 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9830 - accuracy: 0.2670 - val_loss: 1.8972 - val_accuracy: 0.2927 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9365 - accuracy: 0.2904 - val_loss: 1.8155 - val_accuracy: 0.3515 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9111 - accuracy: 0.2997 - val_loss: 1.8301 - val_accuracy: 0.3458 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9051 - accuracy: 0.3010 - val_loss: 1.8883 - val_accuracy: 0.3202 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9115 - accuracy: 0.2981 - val_loss: 1.8488 - val_accuracy: 0.3474 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8767 - accuracy: 0.3135 - val_loss: 1.7708 - val_accuracy: 0.3777 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8572 - accuracy: 0.3190 - val_loss: 1.8070 - val_accuracy: 0.3599 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8680 - accuracy: 0.3182 - val_loss: 1.9074 - val_accuracy: 0.3089 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8854 - accuracy: 0.3130 - val_loss: 1.8272 - val_accuracy: 0.3474 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8796 - accuracy: 0.3111 - val_loss: 1.9119 - val_accuracy: 0.3038 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 1.8980 - accuracy: 0.3077\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8978 - accuracy: 0.3079 - val_loss: 1.8862 - val_accuracy: 0.3320 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8829 - accuracy: 0.3134 - val_loss: 1.8832 - val_accuracy: 0.3408 - lr: 2.0000e-04\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8805 - accuracy: 0.3127 - val_loss: 1.8437 - val_accuracy: 0.3528 - lr: 2.0000e-04\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8778 - accuracy: 0.3148 - val_loss: 1.8500 - val_accuracy: 0.3464 - lr: 2.0000e-04\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8795 - accuracy: 0.3147 - val_loss: 1.8530 - val_accuracy: 0.3479 - lr: 2.0000e-04\n",
            "Epoch 17/100\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 1.8724 - accuracy: 0.3158\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8726 - accuracy: 0.3158 - val_loss: 1.8391 - val_accuracy: 0.3478 - lr: 2.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 with CNN\n",
        "* RMSprop: Adagrad의lr소실 문제를 해결하기 위해 개발된 알고리즘-> 이것으로 하니 val_loss가 말도안되게크다 -> adam으로 수정\n",
        "* Adagrad의 lr소실 문제: global minimum을 빨리 나아가기 위헤서 파라미터의 업데이트 빈도 수에 따라 업데이트 정도 lr을 다르게 해줘야할 필요가 있다.\n",
        "이를 위해 Ada(Adaptive) 알고리즘이 등장했는데 이는 업데이트 빈도 수가 높은 파라미터가 lr보다 작게 업데이트 되는 것이다. 하지만 업데이트 수가 늘어날 수록 lr이 소실된다. \n"
      ],
      "metadata": {
        "id": "R0euvnl5sOYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def solution_model():\n",
        "  cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "  (x_train, y_train),(x_valid, y_valid) = cifar10.load_data()\n",
        "  x_train = x_train/255.0\n",
        "  x_bvalid = x_valid/255.0\n",
        "\n",
        "  model = Sequential([\n",
        "      Conv2D(32, (3,3), activation ='relu', padding = 'same', input_shape=(32,32,3)),\n",
        "      BatchNormalization(),\n",
        "      Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n",
        "      BatchNormalization(),\n",
        "      MaxPooling2D((2,2)),\n",
        "      Dropout(0.2),\n",
        "      Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n",
        "      BatchNormalization(),\n",
        "      Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n",
        "      BatchNormalization(),\n",
        "      MaxPooling2D((2,2)),\n",
        "      Dropout(0.3),\n",
        "      Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n",
        "      BatchNormalization(),\n",
        "      MaxPooling2D((2,2)),\n",
        "      Dropout(0.4),\n",
        "      Flatten(),\n",
        "      Dense(256, activation='relu'),\n",
        "      Dropout(0.5),\n",
        "\n",
        "      Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stop, checkpoint, reduce_lr])\n",
        "\n",
        "  # Load the best model\n",
        "  model.load_weights('best_model.h5')\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "  print('Test loss:', test_loss)\n",
        "  print('Test accuracy:', test_acc)\n",
        "  return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  model = solution_model()\n",
        "  model.save(\"cifar10.h5\")"
      ],
      "metadata": {
        "id": "7T068Y-XovFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc859cab-94b3-433e-98f0-ffc136922f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 10s 7ms/step - loss: 1.7060 - accuracy: 0.3888 - val_loss: 1.5784 - val_accuracy: 0.4862 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.2663 - accuracy: 0.5506 - val_loss: 0.9684 - val_accuracy: 0.6574 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.0716 - accuracy: 0.6257 - val_loss: 0.9157 - val_accuracy: 0.6858 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.9602 - accuracy: 0.6645 - val_loss: 0.7931 - val_accuracy: 0.7224 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.8901 - accuracy: 0.6936 - val_loss: 0.8238 - val_accuracy: 0.7208 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.8257 - accuracy: 0.7148 - val_loss: 0.7621 - val_accuracy: 0.7439 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.7837 - accuracy: 0.7312 - val_loss: 0.6889 - val_accuracy: 0.7712 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.7391 - accuracy: 0.7460 - val_loss: 0.6715 - val_accuracy: 0.7697 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.7029 - accuracy: 0.7601 - val_loss: 0.6741 - val_accuracy: 0.7815 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6806 - accuracy: 0.7667 - val_loss: 0.7128 - val_accuracy: 0.7666 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6579 - accuracy: 0.7740 - val_loss: 0.6203 - val_accuracy: 0.7938 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6292 - accuracy: 0.7854 - val_loss: 0.6242 - val_accuracy: 0.8020 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6086 - accuracy: 0.7916 - val_loss: 0.6887 - val_accuracy: 0.7839 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5921 - accuracy: 0.7967 - val_loss: 0.5726 - val_accuracy: 0.8124 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5732 - accuracy: 0.8059 - val_loss: 0.6466 - val_accuracy: 0.7941 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5570 - accuracy: 0.8093 - val_loss: 0.6124 - val_accuracy: 0.8007 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5432 - accuracy: 0.8141 - val_loss: 0.5637 - val_accuracy: 0.8264 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5251 - accuracy: 0.8196 - val_loss: 0.5514 - val_accuracy: 0.8219 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5138 - accuracy: 0.8223 - val_loss: 0.5433 - val_accuracy: 0.8282 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4986 - accuracy: 0.8281 - val_loss: 0.5226 - val_accuracy: 0.8352 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4866 - accuracy: 0.8332 - val_loss: 0.5108 - val_accuracy: 0.8425 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4727 - accuracy: 0.8385 - val_loss: 0.5376 - val_accuracy: 0.8398 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4690 - accuracy: 0.8383 - val_loss: 0.5254 - val_accuracy: 0.8356 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4603 - accuracy: 0.8421 - val_loss: 0.5594 - val_accuracy: 0.8345 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4507 - accuracy: 0.8452 - val_loss: 0.5046 - val_accuracy: 0.8419 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4370 - accuracy: 0.8489 - val_loss: 0.5506 - val_accuracy: 0.8334 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4329 - accuracy: 0.8509 - val_loss: 0.5190 - val_accuracy: 0.8464 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4274 - accuracy: 0.8537 - val_loss: 0.5248 - val_accuracy: 0.8379 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4132 - accuracy: 0.8569 - val_loss: 0.6595 - val_accuracy: 0.8181 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4048 - accuracy: 0.8599 - val_loss: 0.4661 - val_accuracy: 0.8499 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3980 - accuracy: 0.8619 - val_loss: 0.5140 - val_accuracy: 0.8448 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3929 - accuracy: 0.8635 - val_loss: 0.6014 - val_accuracy: 0.8259 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3931 - accuracy: 0.8644 - val_loss: 0.5069 - val_accuracy: 0.8476 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3824 - accuracy: 0.8684 - val_loss: 0.5470 - val_accuracy: 0.8375 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8699\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3768 - accuracy: 0.8700 - val_loss: 0.5203 - val_accuracy: 0.8461 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3336 - accuracy: 0.8829 - val_loss: 0.4647 - val_accuracy: 0.8642 - lr: 2.0000e-04\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3166 - accuracy: 0.8902 - val_loss: 0.4755 - val_accuracy: 0.8641 - lr: 2.0000e-04\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3079 - accuracy: 0.8921 - val_loss: 0.4669 - val_accuracy: 0.8646 - lr: 2.0000e-04\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3047 - accuracy: 0.8939 - val_loss: 0.4691 - val_accuracy: 0.8655 - lr: 2.0000e-04\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2993 - accuracy: 0.8954 - val_loss: 0.4728 - val_accuracy: 0.8658 - lr: 2.0000e-04\n",
            "Epoch 41/100\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8986\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2904 - accuracy: 0.8987 - val_loss: 0.4734 - val_accuracy: 0.8681 - lr: 2.0000e-04\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2861 - accuracy: 0.8991 - val_loss: 0.4671 - val_accuracy: 0.8689 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2813 - accuracy: 0.9016 - val_loss: 0.4693 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2791 - accuracy: 0.9021 - val_loss: 0.4697 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2785 - accuracy: 0.9029 - val_loss: 0.4642 - val_accuracy: 0.8712 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2772 - accuracy: 0.9027 - val_loss: 0.4680 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2703 - accuracy: 0.9047 - val_loss: 0.4742 - val_accuracy: 0.8685 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2725 - accuracy: 0.9047 - val_loss: 0.4770 - val_accuracy: 0.8687 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2678 - accuracy: 0.9056 - val_loss: 0.4661 - val_accuracy: 0.8689 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2668 - accuracy: 0.9059 - val_loss: 0.4733 - val_accuracy: 0.8682 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2655 - accuracy: 0.9063 - val_loss: 0.4609 - val_accuracy: 0.8707 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2660 - accuracy: 0.9066 - val_loss: 0.4712 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2590 - accuracy: 0.9078 - val_loss: 0.4703 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2610 - accuracy: 0.9073 - val_loss: 0.4709 - val_accuracy: 0.8707 - lr: 1.0000e-04\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2626 - accuracy: 0.9085 - val_loss: 0.4743 - val_accuracy: 0.8696 - lr: 1.0000e-04\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2617 - accuracy: 0.9066 - val_loss: 0.4720 - val_accuracy: 0.8714 - lr: 1.0000e-04\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2588 - accuracy: 0.9090 - val_loss: 0.4670 - val_accuracy: 0.8695 - lr: 1.0000e-04\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2570 - accuracy: 0.9092 - val_loss: 0.4764 - val_accuracy: 0.8691 - lr: 1.0000e-04\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2541 - accuracy: 0.9097 - val_loss: 0.4780 - val_accuracy: 0.8704 - lr: 1.0000e-04\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2513 - accuracy: 0.9119 - val_loss: 0.4822 - val_accuracy: 0.8703 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2503 - accuracy: 0.9123 - val_loss: 0.4818 - val_accuracy: 0.8704 - lr: 1.0000e-04\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4609 - accuracy: 0.8706\n",
            "Test loss: 0.4609261751174927\n",
            "Test accuracy: 0.8705999851226807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "# without CNN\n",
        "\n",
        "* Goal: val_loss <=0.5\n",
        "* prompt:please, code efficient, fast and best performing program with minimum val loss value using python tensorflow\n",
        "from CIFAR10 dataset\n",
        "to perform classification\n",
        "use only dense layers \n",
        "use dropout\n",
        "use batch normalization\n",
        "use checkpoint with save best only, and save weight only \n",
        "use early stopping with patient 10\n",
        "Use reduce lr on plateau\n",
        "set verbose = 1\n",
        "do not use data augmentation\n",
        "and other method to make val loss value lower than 0.5\n",
        "* code_detail:In this implementation, we use three dense layers with dropout and batch normalization to help prevent overfitting. We also use a checkpoint to save the best model weights based on validation loss, early stopping to stop the training process when validation loss stops improving, and a learning rate reduction callback to adjust the learning rate during training based on validation loss. Finally, we evaluate the trained model on the test set and print out the loss and accuracy."
      ],
      "metadata": {
        "id": "TFQFr2zsn3VG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e0561-ad7f-4d7a-a76a-fe46021848d7",
        "id": "oEiK9HWCn3VG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 12s 6ms/step - loss: 1.9237 - accuracy: 0.3179 - val_loss: 1.8647 - val_accuracy: 0.3145 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7509 - accuracy: 0.3719 - val_loss: 1.7515 - val_accuracy: 0.3921 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7091 - accuracy: 0.3889 - val_loss: 1.6563 - val_accuracy: 0.4063 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6829 - accuracy: 0.4022 - val_loss: 1.5977 - val_accuracy: 0.4249 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6663 - accuracy: 0.4030 - val_loss: 1.5948 - val_accuracy: 0.4361 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6477 - accuracy: 0.4128 - val_loss: 1.5985 - val_accuracy: 0.4322 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6379 - accuracy: 0.4183 - val_loss: 1.5245 - val_accuracy: 0.4520 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6343 - accuracy: 0.4170 - val_loss: 1.6037 - val_accuracy: 0.4230 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6325 - accuracy: 0.4173 - val_loss: 1.5463 - val_accuracy: 0.4464 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6228 - accuracy: 0.4249 - val_loss: 1.5915 - val_accuracy: 0.4242 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6260 - accuracy: 0.4179 - val_loss: 1.6534 - val_accuracy: 0.4026 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6108 - accuracy: 0.4273 - val_loss: 1.5925 - val_accuracy: 0.4291 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5928 - accuracy: 0.4329 - val_loss: 1.4545 - val_accuracy: 0.4818 - lr: 2.0000e-04\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5745 - accuracy: 0.4379 - val_loss: 1.4582 - val_accuracy: 0.4823 - lr: 2.0000e-04\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5752 - accuracy: 0.4394 - val_loss: 1.4452 - val_accuracy: 0.4892 - lr: 2.0000e-04\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5673 - accuracy: 0.4394 - val_loss: 1.4391 - val_accuracy: 0.4886 - lr: 2.0000e-04\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5644 - accuracy: 0.4410 - val_loss: 1.4269 - val_accuracy: 0.4964 - lr: 2.0000e-04\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5630 - accuracy: 0.4443 - val_loss: 1.4386 - val_accuracy: 0.4909 - lr: 2.0000e-04\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5595 - accuracy: 0.4451 - val_loss: 1.4437 - val_accuracy: 0.4881 - lr: 2.0000e-04\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5594 - accuracy: 0.4459 - val_loss: 1.4335 - val_accuracy: 0.4883 - lr: 2.0000e-04\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5568 - accuracy: 0.4442 - val_loss: 1.4359 - val_accuracy: 0.4906 - lr: 2.0000e-04\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5567 - accuracy: 0.4453 - val_loss: 1.4312 - val_accuracy: 0.4902 - lr: 2.0000e-04\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5476 - accuracy: 0.4478 - val_loss: 1.4220 - val_accuracy: 0.4934 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5456 - accuracy: 0.4500 - val_loss: 1.4167 - val_accuracy: 0.4983 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5414 - accuracy: 0.4511 - val_loss: 1.4221 - val_accuracy: 0.4928 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5438 - accuracy: 0.4499 - val_loss: 1.4203 - val_accuracy: 0.4947 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5437 - accuracy: 0.4504 - val_loss: 1.4168 - val_accuracy: 0.4948 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5399 - accuracy: 0.4525 - val_loss: 1.4132 - val_accuracy: 0.4972 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5403 - accuracy: 0.4526 - val_loss: 1.4204 - val_accuracy: 0.4965 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5346 - accuracy: 0.4536 - val_loss: 1.4206 - val_accuracy: 0.4993 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5368 - accuracy: 0.4527 - val_loss: 1.4129 - val_accuracy: 0.4987 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5398 - accuracy: 0.4499 - val_loss: 1.4184 - val_accuracy: 0.4996 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5307 - accuracy: 0.4575 - val_loss: 1.4158 - val_accuracy: 0.4971 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5328 - accuracy: 0.4519 - val_loss: 1.4101 - val_accuracy: 0.5003 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5324 - accuracy: 0.4546 - val_loss: 1.4084 - val_accuracy: 0.4972 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5344 - accuracy: 0.4560 - val_loss: 1.4139 - val_accuracy: 0.4954 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5337 - accuracy: 0.4542 - val_loss: 1.4085 - val_accuracy: 0.4975 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5276 - accuracy: 0.4544 - val_loss: 1.4132 - val_accuracy: 0.4979 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5297 - accuracy: 0.4565 - val_loss: 1.4053 - val_accuracy: 0.4986 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5284 - accuracy: 0.4569 - val_loss: 1.4049 - val_accuracy: 0.5021 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5245 - accuracy: 0.4553 - val_loss: 1.4069 - val_accuracy: 0.4988 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5256 - accuracy: 0.4573 - val_loss: 1.4116 - val_accuracy: 0.4977 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5239 - accuracy: 0.4586 - val_loss: 1.4056 - val_accuracy: 0.4983 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5214 - accuracy: 0.4592 - val_loss: 1.4029 - val_accuracy: 0.4999 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5192 - accuracy: 0.4567 - val_loss: 1.4141 - val_accuracy: 0.4947 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5144 - accuracy: 0.4613 - val_loss: 1.3973 - val_accuracy: 0.5023 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5123 - accuracy: 0.4636 - val_loss: 1.3990 - val_accuracy: 0.5035 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5153 - accuracy: 0.4617 - val_loss: 1.4029 - val_accuracy: 0.4984 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5208 - accuracy: 0.4594 - val_loss: 1.4100 - val_accuracy: 0.4966 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5144 - accuracy: 0.4610 - val_loss: 1.3997 - val_accuracy: 0.4996 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5211 - accuracy: 0.4549 - val_loss: 1.4001 - val_accuracy: 0.5030 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5174 - accuracy: 0.4601 - val_loss: 1.3993 - val_accuracy: 0.5004 - lr: 1.0000e-04\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5138 - accuracy: 0.4628 - val_loss: 1.4015 - val_accuracy: 0.4996 - lr: 1.0000e-04\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5132 - accuracy: 0.4610 - val_loss: 1.4054 - val_accuracy: 0.4998 - lr: 1.0000e-04\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5094 - accuracy: 0.4639 - val_loss: 1.3995 - val_accuracy: 0.5023 - lr: 1.0000e-04\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5107 - accuracy: 0.4645 - val_loss: 1.3958 - val_accuracy: 0.4969 - lr: 1.0000e-04\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5105 - accuracy: 0.4590 - val_loss: 1.3978 - val_accuracy: 0.5004 - lr: 1.0000e-04\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5099 - accuracy: 0.4629 - val_loss: 1.4005 - val_accuracy: 0.4999 - lr: 1.0000e-04\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5122 - accuracy: 0.4630 - val_loss: 1.3974 - val_accuracy: 0.5054 - lr: 1.0000e-04\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5077 - accuracy: 0.4616 - val_loss: 1.3911 - val_accuracy: 0.5047 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5077 - accuracy: 0.4664 - val_loss: 1.4014 - val_accuracy: 0.5007 - lr: 1.0000e-04\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5072 - accuracy: 0.4604 - val_loss: 1.4022 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5026 - accuracy: 0.4642 - val_loss: 1.3983 - val_accuracy: 0.4987 - lr: 1.0000e-04\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5051 - accuracy: 0.4627 - val_loss: 1.3999 - val_accuracy: 0.4973 - lr: 1.0000e-04\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5029 - accuracy: 0.4667 - val_loss: 1.3997 - val_accuracy: 0.5009 - lr: 1.0000e-04\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4992 - accuracy: 0.4667 - val_loss: 1.3924 - val_accuracy: 0.5008 - lr: 1.0000e-04\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5049 - accuracy: 0.4673 - val_loss: 1.4002 - val_accuracy: 0.4971 - lr: 1.0000e-04\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5013 - accuracy: 0.4656 - val_loss: 1.3996 - val_accuracy: 0.5006 - lr: 1.0000e-04\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4984 - accuracy: 0.4708 - val_loss: 1.3974 - val_accuracy: 0.5038 - lr: 1.0000e-04\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5006 - accuracy: 0.4684 - val_loss: 1.3886 - val_accuracy: 0.5027 - lr: 1.0000e-04\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4969 - accuracy: 0.4670 - val_loss: 1.3989 - val_accuracy: 0.5011 - lr: 1.0000e-04\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5044 - accuracy: 0.4632 - val_loss: 1.3954 - val_accuracy: 0.5025 - lr: 1.0000e-04\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5011 - accuracy: 0.4651 - val_loss: 1.3957 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5015 - accuracy: 0.4675 - val_loss: 1.3924 - val_accuracy: 0.5034 - lr: 1.0000e-04\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5030 - accuracy: 0.4679 - val_loss: 1.3964 - val_accuracy: 0.4997 - lr: 1.0000e-04\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4918 - accuracy: 0.4678 - val_loss: 1.3974 - val_accuracy: 0.5009 - lr: 1.0000e-04\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4992 - accuracy: 0.4664 - val_loss: 1.3988 - val_accuracy: 0.4990 - lr: 1.0000e-04\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4905 - accuracy: 0.4705 - val_loss: 1.3941 - val_accuracy: 0.5025 - lr: 1.0000e-04\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4913 - accuracy: 0.4702 - val_loss: 1.3857 - val_accuracy: 0.5074 - lr: 1.0000e-04\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4923 - accuracy: 0.4698 - val_loss: 1.3961 - val_accuracy: 0.4998 - lr: 1.0000e-04\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4926 - accuracy: 0.4672 - val_loss: 1.4042 - val_accuracy: 0.4967 - lr: 1.0000e-04\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4947 - accuracy: 0.4701 - val_loss: 1.3956 - val_accuracy: 0.5008 - lr: 1.0000e-04\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4958 - accuracy: 0.4685 - val_loss: 1.3900 - val_accuracy: 0.5009 - lr: 1.0000e-04\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4910 - accuracy: 0.4683 - val_loss: 1.3855 - val_accuracy: 0.5025 - lr: 1.0000e-04\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4947 - accuracy: 0.4673 - val_loss: 1.3914 - val_accuracy: 0.5019 - lr: 1.0000e-04\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.4927 - accuracy: 0.4708 - val_loss: 1.3930 - val_accuracy: 0.5025 - lr: 1.0000e-04\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.4934 - accuracy: 0.4714 - val_loss: 1.3835 - val_accuracy: 0.5034 - lr: 1.0000e-04\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4879 - accuracy: 0.4708 - val_loss: 1.3895 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4872 - accuracy: 0.4706 - val_loss: 1.3875 - val_accuracy: 0.5046 - lr: 1.0000e-04\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4922 - accuracy: 0.4711 - val_loss: 1.3881 - val_accuracy: 0.5019 - lr: 1.0000e-04\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4862 - accuracy: 0.4743 - val_loss: 1.3888 - val_accuracy: 0.5007 - lr: 1.0000e-04\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4892 - accuracy: 0.4711 - val_loss: 1.3935 - val_accuracy: 0.5008 - lr: 1.0000e-04\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4886 - accuracy: 0.4721 - val_loss: 1.3961 - val_accuracy: 0.4992 - lr: 1.0000e-04\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4882 - accuracy: 0.4706 - val_loss: 1.3911 - val_accuracy: 0.5040 - lr: 1.0000e-04\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4846 - accuracy: 0.4727 - val_loss: 1.3965 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4841 - accuracy: 0.4715 - val_loss: 1.3906 - val_accuracy: 0.4997 - lr: 1.0000e-04\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4856 - accuracy: 0.4746 - val_loss: 1.3962 - val_accuracy: 0.4980 - lr: 1.0000e-04\n",
            "Test loss: 1.3835322856903076\n",
            "Test accuracy: 0.5034000277519226\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# load CIFAR-10 dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# normalize pixel values to be between 0 and 1\n",
        "train_data = train_data / 255.0\n",
        "test_data = test_data / 255.0\n",
        "\n",
        "# define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(32*32*3,)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# define the callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model.h5', \n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor='val_loss')\n",
        "early_stop_callback = EarlyStopping(patience=10, monitor='val_loss')\n",
        "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                       patience=5, min_lr=0.0001)\n",
        "\n",
        "# train the model\n",
        "history = model.fit(train_data.reshape(-1, 32*32*3), train_labels, \n",
        "                    epochs=100, \n",
        "                    batch_size=32,\n",
        "                    validation_data=(test_data.reshape(-1, 32*32*3), test_labels),\n",
        "                    callbacks=[checkpoint_callback, early_stop_callback, reduce_lr_callback],\n",
        "                    verbose=1)\n",
        "\n",
        "# load the best saved model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_data.reshape(-1, 32*32*3), test_labels, verbose=0)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "# CNN\n",
        "\n",
        "* Goal: val_loss <=0.5\n",
        "* prompt:please, code efficient, fast and best performing program with minimum val loss value using python tensorflow\n",
        "from CIFAR10 dataset\n",
        "to perform classification\n",
        "use dense layers and CNN\n",
        "use dropout\n",
        "use batch normalization\n",
        "use checkpoint with save best only, and save weight only \n",
        "use early stopping with patient 10\n",
        "Use reduce lr on plateau\n",
        "set verbose = 1\n",
        "do not use data augmentation\n",
        "and other method to make val loss value lower than 0.5\n",
        "* code_detail:\n",
        "1. It uses the CIFAR-10 dataset to perform image classification task.\n",
        "2. The model architecture is a combination of both dense layers and convolutional neural network (CNN) layers. The CNN layers are used for feature extraction, while the dense layers are used for classification.\n",
        "3. Dropout and batch normalization techniques are used to improve the model's generalization performance and reduce overfitting.\n",
        "4. Checkpointing is used to save the best model weights during training, and early stopping is used to stop the training when the validation loss does not improve for 10 consecutive epochs.\n",
        "5. ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss plateaus.\n",
        "6. The target is to achieve the lowest possible validation loss, with a limit of 0.5\n"
      ],
      "metadata": {
        "id": "9CiYzdojn3VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stop, checkpoint, reduce_lr])\n",
        "\n",
        "# Load the best model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3a670b-e85d-4cbf-9e48-ed9f03f798e6",
        "id": "WAEVec28n3VH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 1.6857 - accuracy: 0.3929\n",
            "Epoch 1: val_loss improved from inf to 2.00316, saving model to best_model.h5\n",
            "782/782 [==============================] - 11s 8ms/step - loss: 1.6849 - accuracy: 0.3931 - val_loss: 2.0032 - val_accuracy: 0.4042 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.2198 - accuracy: 0.5691\n",
            "Epoch 2: val_loss improved from 2.00316 to 1.09144, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 1.2197 - accuracy: 0.5692 - val_loss: 1.0914 - val_accuracy: 0.6168 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.6406\n",
            "Epoch 3: val_loss improved from 1.09144 to 0.99649, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.0264 - accuracy: 0.6406 - val_loss: 0.9965 - val_accuracy: 0.6596 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6837\n",
            "Epoch 4: val_loss improved from 0.99649 to 0.87787, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.9180 - accuracy: 0.6839 - val_loss: 0.8779 - val_accuracy: 0.6967 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.8379 - accuracy: 0.7149\n",
            "Epoch 5: val_loss improved from 0.87787 to 0.74428, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.8380 - accuracy: 0.7148 - val_loss: 0.7443 - val_accuracy: 0.7451 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.7803 - accuracy: 0.7330\n",
            "Epoch 6: val_loss did not improve from 0.74428\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.7805 - accuracy: 0.7330 - val_loss: 0.7799 - val_accuracy: 0.7324 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.7543\n",
            "Epoch 7: val_loss improved from 0.74428 to 0.73254, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.7215 - accuracy: 0.7543 - val_loss: 0.7325 - val_accuracy: 0.7558 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.7668\n",
            "Epoch 8: val_loss improved from 0.73254 to 0.66441, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.7669 - val_loss: 0.6644 - val_accuracy: 0.7800 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.7827\n",
            "Epoch 9: val_loss improved from 0.66441 to 0.59306, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6403 - accuracy: 0.7826 - val_loss: 0.5931 - val_accuracy: 0.7991 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7940\n",
            "Epoch 10: val_loss improved from 0.59306 to 0.56574, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6072 - accuracy: 0.7940 - val_loss: 0.5657 - val_accuracy: 0.8154 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.5766 - accuracy: 0.8053\n",
            "Epoch 11: val_loss improved from 0.56574 to 0.55906, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5774 - accuracy: 0.8053 - val_loss: 0.5591 - val_accuracy: 0.8135 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.5466 - accuracy: 0.8142\n",
            "Epoch 12: val_loss improved from 0.55906 to 0.54045, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5466 - accuracy: 0.8143 - val_loss: 0.5405 - val_accuracy: 0.8193 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.8222\n",
            "Epoch 13: val_loss did not improve from 0.54045\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5269 - accuracy: 0.8222 - val_loss: 0.5959 - val_accuracy: 0.8002 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.8299\n",
            "Epoch 14: val_loss improved from 0.54045 to 0.53019, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5024 - accuracy: 0.8299 - val_loss: 0.5302 - val_accuracy: 0.8298 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.8352\n",
            "Epoch 15: val_loss did not improve from 0.53019\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4875 - accuracy: 0.8352 - val_loss: 0.5465 - val_accuracy: 0.8259 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.8422\n",
            "Epoch 16: val_loss improved from 0.53019 to 0.51895, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4665 - accuracy: 0.8420 - val_loss: 0.5190 - val_accuracy: 0.8249 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.8471\n",
            "Epoch 17: val_loss did not improve from 0.51895\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4489 - accuracy: 0.8471 - val_loss: 0.5686 - val_accuracy: 0.8206 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8540\n",
            "Epoch 18: val_loss improved from 0.51895 to 0.49730, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4299 - accuracy: 0.8540 - val_loss: 0.4973 - val_accuracy: 0.8349 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.8575\n",
            "Epoch 19: val_loss improved from 0.49730 to 0.48010, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.4190 - accuracy: 0.8575 - val_loss: 0.4801 - val_accuracy: 0.8461 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.8630\n",
            "Epoch 20: val_loss did not improve from 0.48010\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.4018 - accuracy: 0.8630 - val_loss: 0.5023 - val_accuracy: 0.8410 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.3916 - accuracy: 0.8672\n",
            "Epoch 21: val_loss did not improve from 0.48010\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3918 - accuracy: 0.8672 - val_loss: 0.4927 - val_accuracy: 0.8462 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8682\n",
            "Epoch 22: val_loss improved from 0.48010 to 0.46090, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3824 - accuracy: 0.8682 - val_loss: 0.4609 - val_accuracy: 0.8537 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8742\n",
            "Epoch 23: val_loss did not improve from 0.46090\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3687 - accuracy: 0.8741 - val_loss: 0.4877 - val_accuracy: 0.8474 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8768\n",
            "Epoch 24: val_loss did not improve from 0.46090\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3566 - accuracy: 0.8768 - val_loss: 0.4775 - val_accuracy: 0.8493 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.3465 - accuracy: 0.8807\n",
            "Epoch 25: val_loss did not improve from 0.46090\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3465 - accuracy: 0.8806 - val_loss: 0.4649 - val_accuracy: 0.8527 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8833\n",
            "Epoch 26: val_loss did not improve from 0.46090\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3402 - accuracy: 0.8833 - val_loss: 0.4628 - val_accuracy: 0.8511 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8867\n",
            "Epoch 27: val_loss improved from 0.46090 to 0.45462, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3288 - accuracy: 0.8867 - val_loss: 0.4546 - val_accuracy: 0.8581 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8877\n",
            "Epoch 28: val_loss did not improve from 0.45462\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3275 - accuracy: 0.8876 - val_loss: 0.5065 - val_accuracy: 0.8448 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8917\n",
            "Epoch 29: val_loss did not improve from 0.45462\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3130 - accuracy: 0.8917 - val_loss: 0.4614 - val_accuracy: 0.8570 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8950\n",
            "Epoch 30: val_loss did not improve from 0.45462\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3055 - accuracy: 0.8950 - val_loss: 0.4584 - val_accuracy: 0.8616 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8927\n",
            "Epoch 31: val_loss did not improve from 0.45462\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3107 - accuracy: 0.8927 - val_loss: 0.4655 - val_accuracy: 0.8549 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8981\n",
            "Epoch 32: val_loss did not improve from 0.45462\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2954 - accuracy: 0.8978 - val_loss: 0.4552 - val_accuracy: 0.8608 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9170\n",
            "Epoch 33: val_loss improved from 0.45462 to 0.41972, saving model to best_model.h5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2409 - accuracy: 0.9169 - val_loss: 0.4197 - val_accuracy: 0.8736 - lr: 2.0000e-04\n",
            "Epoch 34/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.2251 - accuracy: 0.9210\n",
            "Epoch 34: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2252 - accuracy: 0.9210 - val_loss: 0.4280 - val_accuracy: 0.8725 - lr: 2.0000e-04\n",
            "Epoch 35/100\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9248\n",
            "Epoch 35: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2132 - accuracy: 0.9248 - val_loss: 0.4367 - val_accuracy: 0.8730 - lr: 2.0000e-04\n",
            "Epoch 36/100\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.2061 - accuracy: 0.9283\n",
            "Epoch 36: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2064 - accuracy: 0.9281 - val_loss: 0.4245 - val_accuracy: 0.8748 - lr: 2.0000e-04\n",
            "Epoch 37/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9287\n",
            "Epoch 37: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2017 - accuracy: 0.9288 - val_loss: 0.4318 - val_accuracy: 0.8746 - lr: 2.0000e-04\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9297\n",
            "Epoch 38: val_loss did not improve from 0.41972\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1981 - accuracy: 0.9297 - val_loss: 0.4364 - val_accuracy: 0.8779 - lr: 2.0000e-04\n",
            "Epoch 39/100\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9338\n",
            "Epoch 39: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1864 - accuracy: 0.9339 - val_loss: 0.4391 - val_accuracy: 0.8756 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9349\n",
            "Epoch 40: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1878 - accuracy: 0.9348 - val_loss: 0.4292 - val_accuracy: 0.8788 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9368\n",
            "Epoch 41: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1793 - accuracy: 0.9368 - val_loss: 0.4331 - val_accuracy: 0.8774 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9375\n",
            "Epoch 42: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1770 - accuracy: 0.9374 - val_loss: 0.4405 - val_accuracy: 0.8776 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9384\n",
            "Epoch 43: val_loss did not improve from 0.41972\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1753 - accuracy: 0.9384 - val_loss: 0.4421 - val_accuracy: 0.8783 - lr: 1.0000e-04\n",
            "Epoch 43: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4197 - accuracy: 0.8736\n",
            "Test loss: 0.4197289049625397\n",
            "Test accuracy: 0.8736000061035156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "# without CNN\n",
        "# need to debug\n",
        "* Goal: val_loss <=0.5\n",
        "* prompt:please, code efficient, fast and best performing program with minimum val loss value using python tensorflow\n",
        "from CIFAR10 dataset\n",
        "to perform classification\n",
        "use only dense layers \n",
        "use dropout\n",
        "use batch normalization\n",
        "use checkpoint with save best only, and save weight only \n",
        "use early stopping with patient 10\n",
        "Use reduce lr on plateau\n",
        "set verbose = 1\n",
        "do not use data augmentation\n",
        "and other method to make val loss value lower than 0.5\n",
        "* code_detail:In this implementation, we use three dense layers with dropout and batch normalization to help prevent overfitting. We also use a checkpoint to save the best model weights based on validation loss, early stopping to stop the training process when validation loss stops improving, and a learning rate reduction callback to adjust the learning rate during training based on validation loss. Finally, we evaluate the trained model on the test set and print out the loss and accuracy."
      ],
      "metadata": {
        "id": "R845U8uUnicT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y_Jwl4DhjqO6",
        "outputId": "73a86829-dcde-4483-abbd-28fbae9a66e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 20s 8ms/step - loss: 1.9339 - accuracy: 0.3114 - val_loss: 1.8961 - val_accuracy: 0.3282 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7555 - accuracy: 0.3699 - val_loss: 1.7553 - val_accuracy: 0.3643 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7259 - accuracy: 0.3830 - val_loss: 1.7014 - val_accuracy: 0.3918 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6959 - accuracy: 0.3914 - val_loss: 1.6869 - val_accuracy: 0.3864 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6949 - accuracy: 0.3956 - val_loss: 1.5665 - val_accuracy: 0.4467 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7126 - accuracy: 0.3878 - val_loss: 1.6719 - val_accuracy: 0.3970 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.6861 - accuracy: 0.3963 - val_loss: 1.6493 - val_accuracy: 0.4136 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6741 - accuracy: 0.4002 - val_loss: 1.6854 - val_accuracy: 0.4004 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6541 - accuracy: 0.4109 - val_loss: 1.5579 - val_accuracy: 0.4407 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6408 - accuracy: 0.4144 - val_loss: 1.5396 - val_accuracy: 0.4534 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6407 - accuracy: 0.4161 - val_loss: 1.6102 - val_accuracy: 0.4099 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6452 - accuracy: 0.4122 - val_loss: 1.6036 - val_accuracy: 0.4221 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6396 - accuracy: 0.4147 - val_loss: 1.5307 - val_accuracy: 0.4511 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6389 - accuracy: 0.4165 - val_loss: 1.5803 - val_accuracy: 0.4378 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.6309 - accuracy: 0.4213 - val_loss: 1.5469 - val_accuracy: 0.4518 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6231 - accuracy: 0.4208 - val_loss: 1.5073 - val_accuracy: 0.4612 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6194 - accuracy: 0.4231 - val_loss: 1.5311 - val_accuracy: 0.4546 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6164 - accuracy: 0.4228 - val_loss: 1.5252 - val_accuracy: 0.4466 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6064 - accuracy: 0.4252 - val_loss: 1.5102 - val_accuracy: 0.4534 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6060 - accuracy: 0.4239 - val_loss: 1.5111 - val_accuracy: 0.4602 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6053 - accuracy: 0.4282 - val_loss: 1.5061 - val_accuracy: 0.4630 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5977 - accuracy: 0.4310 - val_loss: 1.4949 - val_accuracy: 0.4671 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5978 - accuracy: 0.4293 - val_loss: 1.4793 - val_accuracy: 0.4727 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5964 - accuracy: 0.4316 - val_loss: 1.4619 - val_accuracy: 0.4820 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5904 - accuracy: 0.4328 - val_loss: 1.5048 - val_accuracy: 0.4585 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5880 - accuracy: 0.4344 - val_loss: 1.4910 - val_accuracy: 0.4644 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5813 - accuracy: 0.4351 - val_loss: 1.5027 - val_accuracy: 0.4650 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5858 - accuracy: 0.4354 - val_loss: 1.4640 - val_accuracy: 0.4753 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5845 - accuracy: 0.4351 - val_loss: 1.5020 - val_accuracy: 0.4694 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5534 - accuracy: 0.4482 - val_loss: 1.4242 - val_accuracy: 0.4940 - lr: 2.0000e-04\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5491 - accuracy: 0.4488 - val_loss: 1.4373 - val_accuracy: 0.4874 - lr: 2.0000e-04\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5419 - accuracy: 0.4509 - val_loss: 1.4278 - val_accuracy: 0.4855 - lr: 2.0000e-04\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5464 - accuracy: 0.4487 - val_loss: 1.4223 - val_accuracy: 0.4870 - lr: 2.0000e-04\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5463 - accuracy: 0.4498 - val_loss: 1.4235 - val_accuracy: 0.4894 - lr: 2.0000e-04\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 1.5408 - accuracy: 0.4524 - val_loss: 1.4299 - val_accuracy: 0.4865 - lr: 2.0000e-04\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5363 - accuracy: 0.4537 - val_loss: 1.4215 - val_accuracy: 0.4880 - lr: 2.0000e-04\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5371 - accuracy: 0.4536 - val_loss: 1.4239 - val_accuracy: 0.4885 - lr: 2.0000e-04\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5389 - accuracy: 0.4516 - val_loss: 1.4163 - val_accuracy: 0.4893 - lr: 2.0000e-04\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5416 - accuracy: 0.4492 - val_loss: 1.4128 - val_accuracy: 0.4951 - lr: 2.0000e-04\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5286 - accuracy: 0.4565 - val_loss: 1.4158 - val_accuracy: 0.4924 - lr: 2.0000e-04\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5401 - accuracy: 0.4527 - val_loss: 1.4241 - val_accuracy: 0.4870 - lr: 2.0000e-04\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5343 - accuracy: 0.4559 - val_loss: 1.4164 - val_accuracy: 0.4975 - lr: 2.0000e-04\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5342 - accuracy: 0.4552 - val_loss: 1.4125 - val_accuracy: 0.4924 - lr: 2.0000e-04\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5342 - accuracy: 0.4550 - val_loss: 1.4235 - val_accuracy: 0.4903 - lr: 2.0000e-04\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5374 - accuracy: 0.4516 - val_loss: 1.4224 - val_accuracy: 0.4905 - lr: 2.0000e-04\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5322 - accuracy: 0.4561 - val_loss: 1.4101 - val_accuracy: 0.4926 - lr: 2.0000e-04\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5298 - accuracy: 0.4570 - val_loss: 1.4157 - val_accuracy: 0.4916 - lr: 2.0000e-04\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5309 - accuracy: 0.4553 - val_loss: 1.4265 - val_accuracy: 0.4895 - lr: 2.0000e-04\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5283 - accuracy: 0.4585 - val_loss: 1.4210 - val_accuracy: 0.4938 - lr: 2.0000e-04\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5252 - accuracy: 0.4572 - val_loss: 1.4167 - val_accuracy: 0.4938 - lr: 2.0000e-04\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5289 - accuracy: 0.4558 - val_loss: 1.4220 - val_accuracy: 0.4906 - lr: 2.0000e-04\n",
            "Epoch 52/100\n",
            " 218/1563 [===>..........................] - ETA: 7s - loss: 1.5414 - accuracy: 0.4526"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c3142f6b4127>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m history = model.fit(train_data.reshape(-1, 32*32*3), train_labels, \n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# load CIFAR-10 dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# normalize pixel values to be between 0 and 1\n",
        "train_data = train_data / 255.0\n",
        "test_data = test_data / 255.0\n",
        "\n",
        "# define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(32*32*3,)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# define the callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model.h5', \n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor='val_loss')\n",
        "early_stop_callback = EarlyStopping(patience=10, monitor='val_loss')\n",
        "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                       patience=5, min_lr=0.0001)\n",
        "\n",
        "# train the model\n",
        "history = model.fit(train_data.reshape(-1, 32*32*3), train_labels, \n",
        "                    epochs=100, \n",
        "                    batch_size=32,\n",
        "                    validation_data=(test_data.reshape(-1, 32*32*3), test_labels),\n",
        "                    callbacks=[checkpoint_callback, early_stop_callback, reduce_lr_callback],\n",
        "                    verbose=1)\n",
        "\n",
        "# load the best saved model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_data.reshape(-1, 32*32*3), test_labels, verbose=0)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ]
    }
  ]
}