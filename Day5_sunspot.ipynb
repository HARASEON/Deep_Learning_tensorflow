{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMULC1UJCwc43SsxT8v+iRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day5_sunspot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sunspot\n",
        "1) Lambda : Lambda 함수는 단순히 예측값에 400을 곱하므로 val_loss가 커짐\n",
        "  * why? 인공신경망에 keras가 제공하지 않는 레이어를 새로 만들기 위해서\n",
        "  * ex. 차원확장. 스케일업 등\n",
        "  \n",
        "2) No lambda\n",
        "* 데이터셋 특징:\n",
        "  * Training data: 3000ea\n",
        "  * Validation data = 235ea"
      ],
      "metadata": {
        "id": "FpGeHQ0Gi3lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time series data\n",
        "* 예시 : 공장/건강/운동/환자 센서데이터, 주식, 날씨(강우량, 온도, 습도,풍속, 기압 등),기업매출액/매출품목/주문량/생산량/고객만족도, 기업 분기/연차보고서\n",
        "* 시계열 데이터의 정답은 어떻게 만들어지는가?\n",
        "  * 데이터를 묶어서(window) 바로 다음 데이터를 정답(label)으로 삼고 다음 데이터의 예측구간(horizon)을 예측한다.\n",
        "* 사용 예시: \n",
        "  * 시계열 데이터들 법칙에서 따라서 다음 생성되는 데이터를 예측\n",
        "  * 수요예측, 강우량예측, 온도예측\n",
        "  * 자동차 센서데이터에 의한 기계고장 예측\n",
        "  * 엔진 예지정비\n",
        "* 전처리: window + horizon, shuffling = k-fold cross validation\n",
        "  * why? 데이터를 model이 모두 외우기 때문에 이를 방지하기 위해서"
      ],
      "metadata": {
        "id": "RtomPF8yi_Qj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RkQJmDOAi0lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747bd1c1-e84d-42fc-a8f9-f8e76b96a3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.10.0\n",
            "Uninstalling tensorflow-2.10.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.10.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.10.0\n",
            "  Using cached tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (67.6.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (16.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.10.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.53.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sunspot-Lambda\n",
        "* 손코딩\n",
        "* target val_loss: 13.3\n",
        "* 마지막 dense에 왜 1이 들어가고 relu가 안들어가는지 궁금해서 아래를 비교해보았다.\n",
        "  * Dense(1),\n",
        "  * Dense(6, relu) -> val_loss가 위보다 더 낮게 나왔다."
      ],
      "metadata": {
        "id": "uIUZ5dGev8wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "  series = tf.expand_dims(series, axis = -1)\n",
        "  ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "  ds = ds.window(window_size + 1, shift = 1, drop_remainder = True)\n",
        "  ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "  ds = ds.shuffle(shuffle_buffer)\n",
        "  ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "  return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "def solution_model():\n",
        "  url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "  urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "  time_step = []\n",
        "  sunspots = []\n",
        "\n",
        "  with open('sunspots.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter = ',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      sunspots.append(float(row[2]))\n",
        "      time_step.append(int(row[0]))\n",
        "\n",
        "    series = np.array(sunspots)\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    split_time = 3000\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set  = windowed_dataset(x_train, window_size = window_size, batch_size = batch_size, shuffle_buffer = shuffle_buffer_size)\n",
        "\n",
        "    validation_set= windowed_dataset(x_valid, window_size = window_size, batch_size = batch_size, shuffle_buffer = shuffle_buffer_size)\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=16, kernel_size=5,\n",
        "               padding = 'causal',\n",
        "               activation = 'relu',\n",
        "               input_shape =[None, 1]),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation = \"relu\"),\n",
        "        Dense(32, activation = \"relu\"),\n",
        "        Dense(1),\n",
        "        Lambda(lambda x :x*400)\n",
        "    ])\n",
        "    \n",
        "    optimizer= SGD(learning_rate = 1e-4, momentum = 0.9)\n",
        "    loss = Huber()\n",
        "    model.compile(loss = loss,\n",
        "                  optimizer = optimizer,\n",
        "                  metrics =[\"mae\"])\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "      save_weights_only = True,\n",
        "      save_best_only = True, \n",
        "      monitor = 'val_mae',\n",
        "      verbose = 1)\n",
        "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "    epochs = 150\n",
        "    history = model.fit(train_set,\n",
        "                        validation_data = (validation_set),\n",
        "                        epochs = epochs,\n",
        "                        callbacks =[checkpoint, early_stopping])\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  model = solution_model()\n",
        "  model.save(\"sunspot_lambda.h5\")\n",
        "\n",
        "  from google.colab import files\n",
        "  files.download('sunspot_lambda.h5')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cve-LPqpv7fa",
        "outputId": "81561cff-db76-4fd8-8ada-c36d51b7c338"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "     92/Unknown - 9s 15ms/step - loss: 38.4115 - mae: 38.9076\n",
            "Epoch 1: val_mae improved from inf to 17.12928, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 10s 34ms/step - loss: 38.2428 - mae: 38.7389 - val_loss: 16.6357 - val_mae: 17.1293\n",
            "Epoch 2/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 20.9220 - mae: 21.4158\n",
            "Epoch 2: val_mae improved from 17.12928 to 12.33761, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 20.9060 - mae: 21.3997 - val_loss: 11.8449 - val_mae: 12.3376\n",
            "Epoch 3/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 17.4893 - mae: 17.9819\n",
            "Epoch 3: val_mae did not improve from 12.33761\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 17.4893 - mae: 17.9819 - val_loss: 12.5631 - val_mae: 13.0536\n",
            "Epoch 4/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 15.7663 - mae: 16.2580\n",
            "Epoch 4: val_mae improved from 12.33761 to 10.08084, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 15.7619 - mae: 16.2537 - val_loss: 9.5950 - val_mae: 10.0808\n",
            "Epoch 5/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 16.6068 - mae: 17.0988\n",
            "Epoch 5: val_mae did not improve from 10.08084\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 16.5871 - mae: 17.0791 - val_loss: 10.4969 - val_mae: 10.9847\n",
            "Epoch 6/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 14.9426 - mae: 15.4342\n",
            "Epoch 6: val_mae improved from 10.08084 to 9.87954, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 14.9336 - mae: 15.4253 - val_loss: 9.3946 - val_mae: 9.8795\n",
            "Epoch 7/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 15.5250 - mae: 16.0158\n",
            "Epoch 7: val_mae did not improve from 9.87954\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 15.6420 - mae: 16.1330 - val_loss: 16.1314 - val_mae: 16.6266\n",
            "Epoch 8/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 16.2582 - mae: 16.7504\n",
            "Epoch 8: val_mae did not improve from 9.87954\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 16.2932 - mae: 16.7854 - val_loss: 10.9290 - val_mae: 11.4187\n",
            "Epoch 9/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 14.1120 - mae: 14.6029\n",
            "Epoch 9: val_mae did not improve from 9.87954\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 14.1013 - mae: 14.5922 - val_loss: 9.6808 - val_mae: 10.1685\n",
            "Epoch 10/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 13.7516 - mae: 14.2423\n",
            "Epoch 10: val_mae did not improve from 9.87954\n",
            "93/93 [==============================] - 3s 27ms/step - loss: 13.7643 - mae: 14.2550 - val_loss: 17.7109 - val_mae: 18.2039\n",
            "Epoch 11/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 14.8712 - mae: 15.3621\n",
            "Epoch 11: val_mae did not improve from 9.87954\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 14.8712 - mae: 15.3621 - val_loss: 12.2655 - val_mae: 12.7565\n",
            "Epoch 12/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 13.0695 - mae: 13.5600\n",
            "Epoch 12: val_mae improved from 9.87954 to 9.19967, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 13.0695 - mae: 13.5600 - val_loss: 8.7138 - val_mae: 9.1997\n",
            "Epoch 13/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 13.1667 - mae: 13.6572\n",
            "Epoch 13: val_mae did not improve from 9.19967\n",
            "93/93 [==============================] - 1s 15ms/step - loss: 13.1625 - mae: 13.6530 - val_loss: 16.4548 - val_mae: 16.9474\n",
            "Epoch 14/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 12.3609 - mae: 12.8504\n",
            "Epoch 14: val_mae improved from 9.19967 to 7.41823, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 12.3451 - mae: 12.8345 - val_loss: 6.9389 - val_mae: 7.4182\n",
            "Epoch 15/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 12.2240 - mae: 12.7134\n",
            "Epoch 15: val_mae improved from 7.41823 to 7.36326, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 12.2964 - mae: 12.7859 - val_loss: 6.8815 - val_mae: 7.3633\n",
            "Epoch 16/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 12.2574 - mae: 12.7470\n",
            "Epoch 16: val_mae did not improve from 7.36326\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 12.2108 - mae: 12.7003 - val_loss: 8.6238 - val_mae: 9.1105\n",
            "Epoch 17/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 13.3790 - mae: 13.8689\n",
            "Epoch 17: val_mae did not improve from 7.36326\n",
            "93/93 [==============================] - 2s 15ms/step - loss: 13.3790 - mae: 13.8689 - val_loss: 11.2775 - val_mae: 11.7728\n",
            "Epoch 18/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 14.1017 - mae: 14.5925\n",
            "Epoch 18: val_mae improved from 7.36326 to 7.03197, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 14.1017 - mae: 14.5925 - val_loss: 6.5491 - val_mae: 7.0320\n",
            "Epoch 19/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 13.4392 - mae: 13.9293\n",
            "Epoch 19: val_mae did not improve from 7.03197\n",
            "93/93 [==============================] - 1s 14ms/step - loss: 13.6576 - mae: 14.1479 - val_loss: 8.4410 - val_mae: 8.9306\n",
            "Epoch 20/150\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 12.3896 - mae: 12.8792\n",
            "Epoch 20: val_mae improved from 7.03197 to 6.18050, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 12.3884 - mae: 12.8780 - val_loss: 5.7022 - val_mae: 6.1805\n",
            "Epoch 21/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 13.0162 - mae: 13.5056\n",
            "Epoch 21: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 25ms/step - loss: 13.0162 - mae: 13.5056 - val_loss: 7.8253 - val_mae: 8.3098\n",
            "Epoch 22/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 12.3650 - mae: 12.8549\n",
            "Epoch 22: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 12.4875 - mae: 12.9775 - val_loss: 7.2354 - val_mae: 7.7225\n",
            "Epoch 23/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 15.6978 - mae: 16.1903\n",
            "Epoch 23: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 15.6978 - mae: 16.1903 - val_loss: 5.7509 - val_mae: 6.2295\n",
            "Epoch 24/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 15.3855 - mae: 15.8776\n",
            "Epoch 24: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 15.3859 - mae: 15.8780 - val_loss: 6.9746 - val_mae: 7.4606\n",
            "Epoch 25/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 12.2379 - mae: 12.7271\n",
            "Epoch 25: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 12.2185 - mae: 12.7077 - val_loss: 9.7321 - val_mae: 10.2233\n",
            "Epoch 26/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 12.9342 - mae: 13.4234\n",
            "Epoch 26: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 1s 15ms/step - loss: 13.1040 - mae: 13.5933 - val_loss: 8.9884 - val_mae: 9.4830\n",
            "Epoch 27/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 10.3820 - mae: 10.8695\n",
            "Epoch 27: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 10.3820 - mae: 10.8695 - val_loss: 18.0581 - val_mae: 18.5498\n",
            "Epoch 28/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 13.2094 - mae: 13.6993\n",
            "Epoch 28: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 13.1912 - mae: 13.6812 - val_loss: 9.7489 - val_mae: 10.2360\n",
            "Epoch 29/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 14.6273 - mae: 15.1182\n",
            "Epoch 29: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 14.5258 - mae: 15.0167 - val_loss: 12.4745 - val_mae: 12.9719\n",
            "Epoch 30/150\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 10.3208 - mae: 10.8084\n",
            "Epoch 30: val_mae did not improve from 6.18050\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 10.3902 - mae: 10.8779 - val_loss: 19.8366 - val_mae: 20.3362\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7abebf76-93ab-4e81-b2a8-509ec7136ec8\", \"sunspot_lambda.h5\", 1271376)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "* prompt:\n",
        "code with the dataset from 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv' .  use Lambda(lambda predicted_value :predicted_value*400), RNN and transfer learning to get val_loss under 13.3\n",
        "\n",
        "* responds:\n",
        "pretrain 모델을 사용하려 해서 실패패"
      ],
      "metadata": {
        "id": "Sl7pN5NVAFBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the data\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "sunspots_df = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "# Prepare the data for training and testing\n",
        "train_size = int(len(sunspots_df) * 0.8)\n",
        "train_df = sunspots_df.iloc[:train_size]\n",
        "test_df = sunspots_df.iloc[train_size:]\n",
        "\n",
        "# Normalize the data\n",
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std\n",
        "\n",
        "# Define a function to create sequences from the data\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(seq_length, len(data)):\n",
        "        X.append(data[i-seq_length:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Define the sequence length\n",
        "seq_length = 30\n",
        "\n",
        "# Create the training and testing sequences\n",
        "train_X, train_y = create_sequences(train_df.values, seq_length)\n",
        "test_X, test_y = create_sequences(test_df.values, seq_length)\n",
        "\n",
        "# Load a pre-trained model\n",
        "base_model = keras.models.load_model('/path/to/pretrained/model.h5')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a Lambda layer to the model\n",
        "model = keras.Sequential([\n",
        "    base_model,\n",
        "    layers.Lambda(lambda predicted_value: predicted_value * 400)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error',\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_X, train_y,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(test_X, test_y),\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"Validation loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "b1NQHCQlAHU7",
        "outputId": "6d7bfe06-9c4d-4278-fc99-29d82b6a88a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0bb45f9b250b>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Load a pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/pretrained/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /path/to/pretrained/model.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pretrained model 없이 다시 시도\n",
        "\n",
        "* loss : mse를 사용한 것을 볼 수 있다.\n",
        "  * huber loss를 썼던 위 코드보다 mse가 sunspot dataset에는 val_loss가 10 이상 낮게 나왔다.\n",
        "* callback함수를 쓰라는 명령을 내려서 뒷부분을 수정했다.\n",
        "\n"
      ],
      "metadata": {
        "id": "np3ODPBqBXoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the data\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "sunspots_df = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "# Prepare the data for training and testing \n",
        "# train : test = 8:2\n",
        "train_size = int(len(sunspots_df) * 0.8)\n",
        "train_df = sunspots_df.iloc[:train_size]\n",
        "test_df = sunspots_df.iloc[train_size:]\n",
        "\n",
        "# Normalize the data\n",
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std\n",
        "\n",
        "# Define a function to create sequences from the data\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        # Extract the sequence of length seq_length\n",
        "        seq = data[i : i + seq_length + 1]\n",
        "\n",
        "        # Split the sequence into features (input) and target (output)\n",
        "        X.append(seq[:-1, 1]) # select only the second column for each row to create the sequence\n",
        "        y.append(seq[-1, 1])\n",
        "\n",
        "    # Convert the lists to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Reshape the input data to have shape (num_samples, seq_length, input_dim)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Define the sequence length\n",
        "seq_length = 30\n",
        "\n",
        "# Create the training and testing sequences\n",
        "train_X, train_y = create_sequences(train_df.values, seq_length)\n",
        "test_X, test_y = create_sequences(test_df.values, seq_length)\n",
        "\n",
        "# Define the RNN model\n",
        "model = keras.Sequential([\n",
        "    layers.LSTM(64, input_shape=(seq_length, 1)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "    layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error',\n",
        ")\n",
        "\n",
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor = 'val_loss',save_best_only = True)\n",
        "checkpoint_path = 'checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "    save_weights_only=True, \n",
        "    save_best_only=True, \n",
        "    monitor='val_loss', \n",
        "    verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_X, train_y,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(test_X, test_y),\n",
        "    callbacks= [early_stopping, checkpoint],\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "model.load_weights(checkpoint_path)\n",
        "model.save('sunspot_lambda.h5')\n",
        "# Evaluate the model\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"Validation loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVImAdKBbHK",
        "outputId": "cb17f173-6b9d-4930-f49a-c6b0f045d053"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 40.4002\n",
            "Epoch 1: val_loss improved from inf to 1.15531, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 4s 16ms/step - loss: 37.0060 - val_loss: 1.1553\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.7452\n",
            "Epoch 2: val_loss improved from 1.15531 to 0.45906, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.7452 - val_loss: 0.4591\n",
            "Epoch 3/100\n",
            "72/80 [==========================>...] - ETA: 0s - loss: 0.4732\n",
            "Epoch 3: val_loss improved from 0.45906 to 0.36965, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.4747 - val_loss: 0.3697\n",
            "Epoch 4/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 0.3408\n",
            "Epoch 4: val_loss improved from 0.36965 to 0.28212, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3465 - val_loss: 0.2821\n",
            "Epoch 5/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 0.3340\n",
            "Epoch 5: val_loss improved from 0.28212 to 0.20783, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3254 - val_loss: 0.2078\n",
            "Epoch 6/100\n",
            "71/80 [=========================>....] - ETA: 0s - loss: 0.2628\n",
            "Epoch 6: val_loss did not improve from 0.20783\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.2430\n",
            "Epoch 7/100\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.2925\n",
            "Epoch 7: val_loss improved from 0.20783 to 0.16245, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2913 - val_loss: 0.1624\n",
            "Epoch 8/100\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.2484\n",
            "Epoch 8: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.2123\n",
            "Epoch 9/100\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.2399\n",
            "Epoch 9: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2395 - val_loss: 0.1734\n",
            "Epoch 10/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 0.2791\n",
            "Epoch 10: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2865 - val_loss: 0.2996\n",
            "Epoch 11/100\n",
            "72/80 [==========================>...] - ETA: 0s - loss: 0.3413\n",
            "Epoch 11: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3307 - val_loss: 0.3302\n",
            "Epoch 12/100\n",
            "71/80 [=========================>....] - ETA: 0s - loss: 0.6204\n",
            "Epoch 12: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.6457 - val_loss: 1.0134\n",
            "Epoch 13/100\n",
            "75/80 [===========================>..] - ETA: 0s - loss: 0.9272\n",
            "Epoch 13: val_loss did not improve from 0.16245\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8843 - val_loss: 0.2822\n",
            "Epoch 14/100\n",
            "72/80 [==========================>...] - ETA: 0s - loss: 0.2417\n",
            "Epoch 14: val_loss improved from 0.16245 to 0.15969, saving model to checkpoint.ckpt\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.1597\n",
            "Epoch 15/100\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.2936\n",
            "Epoch 15: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2946 - val_loss: 0.1738\n",
            "Epoch 16/100\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.2537\n",
            "Epoch 16: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2533 - val_loss: 0.2843\n",
            "Epoch 17/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 0.2977\n",
            "Epoch 17: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2946 - val_loss: 0.2477\n",
            "Epoch 18/100\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.3135\n",
            "Epoch 18: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3205 - val_loss: 0.4121\n",
            "Epoch 19/100\n",
            "72/80 [==========================>...] - ETA: 0s - loss: 0.5900\n",
            "Epoch 19: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.5834 - val_loss: 0.8367\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.7918\n",
            "Epoch 20: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.7918 - val_loss: 0.2479\n",
            "Epoch 21/100\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.2023\n",
            "Epoch 21: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.2095 - val_loss: 0.2420\n",
            "Epoch 22/100\n",
            "75/80 [===========================>..] - ETA: 0s - loss: 0.6185\n",
            "Epoch 22: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.6059 - val_loss: 0.6914\n",
            "Epoch 23/100\n",
            "73/80 [==========================>...] - ETA: 0s - loss: 0.6746\n",
            "Epoch 23: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.6575 - val_loss: 0.8641\n",
            "Epoch 24/100\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.3334\n",
            "Epoch 24: val_loss did not improve from 0.15969\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.3306 - val_loss: 0.1792\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_5 (LSTM)               (None, 64)                16896     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " lambda_4 (Lambda)           (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,009\n",
            "Trainable params: 19,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Validation loss: 0.1792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sunspot - NO Lambda\n",
        "* 손코딩\n",
        "* val_loss : < 0.07 success! 0.03\n",
        "* 궁금한것 : 왜 huber loss를 사용하는 데 metric에 mae를 사용해야만 하나요? 에러메시지는 그냥 matricfunction에 없어서 그렇다고 하는데 만들면 비효율적이어서 안만든 것인가요? mae가 다 설명할 수 있어서 굳이 만들지 않은 건가요?\n",
        "\n"
      ],
      "metadata": {
        "id": "Zfb3X5emHoaz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3_9wrbxeB9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e59a66e0-da18-4e50-8f96-a2a0f1d42b74"
      },
      "source": [
        "# QUESTION\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv dataset.\n",
        "# Your neural network must  have an MAE\n",
        "# of 0.12 or less on the normalized dataset for top marks.\n",
        "# Code for normalizing the data is provided and should not be changed.\n",
        "# At the bottom of this file, we provide  some testing\n",
        "# code should you want to check your model.\n",
        "\n",
        "# Note: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            sunspots.append(float(row[2])) # YOUR CODE HERE)\n",
        "            time_step.append(int(row[0]))  # YOUR CODE HERE)\n",
        "\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # The data should be split into training and validation sets at time step 3000\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "\n",
        "    train_set = windowed_dataset(x_train, \n",
        "        window_size=window_size, \n",
        "        batch_size=batch_size, \n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation=\"relu\"),     \n",
        "    \n",
        "                                              \n",
        "      # YOUR CODE HERE. Whatever your first layer is, the input shape will be [None,1] when using the Windowed_dataset above, depending on the layer type chosen\n",
        "      tf.keras.layers.Dense(1, activation = \"relu\")\n",
        "    ])\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE\n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
        "    # \t [[{{node IteratorGetNext}}]]\n",
        "    #\n",
        "\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        " #   optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"mae\"]) # \"huber\"가 안되는 이유?\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    epochs=100\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=(validation_set), \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, this .h5 model will be\n",
        "# sent to the testing infrastructure for scoring.\n",
        "\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in each category before you finally submit your exam.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"sunspot_nolambda.h5\")\n",
        "\n",
        "# 시험볼때는 이줄 아래는 복사하지 마세요\n",
        "    from google.colab import files\n",
        "    files.download('sunspot_nolambda.h5')\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     93/Unknown - 14s 20ms/step - loss: 0.0076 - mae: 0.0914\n",
            "Epoch 1: val_mae improved from inf to 0.04614, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 16s 45ms/step - loss: 0.0076 - mae: 0.0914 - val_loss: 0.0021 - val_mae: 0.0461\n",
            "Epoch 2/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0606\n",
            "Epoch 2: val_mae improved from 0.04614 to 0.04318, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0034 - mae: 0.0605 - val_loss: 0.0019 - val_mae: 0.0432\n",
            "Epoch 3/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0541\n",
            "Epoch 3: val_mae improved from 0.04318 to 0.04046, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 31ms/step - loss: 0.0028 - mae: 0.0543 - val_loss: 0.0018 - val_mae: 0.0405\n",
            "Epoch 4/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0029 - mae: 0.0557\n",
            "Epoch 4: val_mae improved from 0.04046 to 0.03985, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0029 - mae: 0.0557 - val_loss: 0.0018 - val_mae: 0.0399\n",
            "Epoch 5/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0030 - mae: 0.0563\n",
            "Epoch 5: val_mae improved from 0.03985 to 0.03878, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0029 - mae: 0.0562 - val_loss: 0.0016 - val_mae: 0.0388\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0550\n",
            "Epoch 6: val_mae improved from 0.03878 to 0.03786, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0028 - mae: 0.0550 - val_loss: 0.0016 - val_mae: 0.0379\n",
            "Epoch 7/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0028 - mae: 0.0540\n",
            "Epoch 7: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0027 - mae: 0.0540 - val_loss: 0.0017 - val_mae: 0.0397\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0530\n",
            "Epoch 8: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 3s 28ms/step - loss: 0.0026 - mae: 0.0530 - val_loss: 0.0023 - val_mae: 0.0448\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0529\n",
            "Epoch 9: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0026 - mae: 0.0529 - val_loss: 0.0020 - val_mae: 0.0412\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0526\n",
            "Epoch 10: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.0025 - mae: 0.0526 - val_loss: 0.0025 - val_mae: 0.0485\n",
            "Epoch 11/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0515\n",
            "Epoch 11: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.0025 - mae: 0.0516 - val_loss: 0.0017 - val_mae: 0.0420\n",
            "Epoch 12/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0507\n",
            "Epoch 12: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 3s 31ms/step - loss: 0.0024 - mae: 0.0508 - val_loss: 0.0015 - val_mae: 0.0380\n",
            "Epoch 13/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0500\n",
            "Epoch 13: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.0023 - mae: 0.0501 - val_loss: 0.0021 - val_mae: 0.0448\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0526\n",
            "Epoch 14: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0026 - mae: 0.0526 - val_loss: 0.0027 - val_mae: 0.0479\n",
            "Epoch 15/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0026 - mae: 0.0523\n",
            "Epoch 15: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0026 - mae: 0.0525 - val_loss: 0.0023 - val_mae: 0.0459\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0508\n",
            "Epoch 16: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 3s 30ms/step - loss: 0.0024 - mae: 0.0508 - val_loss: 0.0025 - val_mae: 0.0457\n",
            "Epoch 17/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0023 - mae: 0.0496\n",
            "Epoch 17: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.0023 - mae: 0.0497 - val_loss: 0.0017 - val_mae: 0.0443\n",
            "Epoch 18/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0517\n",
            "Epoch 18: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0024 - mae: 0.0515 - val_loss: 0.0020 - val_mae: 0.0421\n",
            "Epoch 19/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0518\n",
            "Epoch 19: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0025 - mae: 0.0520 - val_loss: 0.0022 - val_mae: 0.0443\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0023 - mae: 0.0498\n",
            "Epoch 20: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 3s 30ms/step - loss: 0.0023 - mae: 0.0498 - val_loss: 0.0027 - val_mae: 0.0476\n",
            "Epoch 21/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0520\n",
            "Epoch 21: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0025 - mae: 0.0519 - val_loss: 0.0023 - val_mae: 0.0454\n",
            "Epoch 22/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0501\n",
            "Epoch 22: val_mae did not improve from 0.03786\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.0024 - mae: 0.0502 - val_loss: 0.0019 - val_mae: 0.0402\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0bab3620-7213-4547-a372-8ed1c1aa1b8d\", \"sunspot_nolambda.h5\", 1043456)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "* val_loss < 0.07 fail 0.11 -> 모델이 dense 하나에 LSTM하나다.\n",
        "* prompt: code RNN and transfer model with the dataset from url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv' and use the callback, checkpoint,and EarlyStopping. save the best model into '.h5' file  to get lower val_loss under 0.07 without any error. \n",
        "* response: \n",
        "1. Loading and preprocessing the data: The code loads the sunspot dataset from a URL using pandas, and then preprocesses the data by normalizing it. Normalizing the data helps to ensure that each feature is on a similar scale, which can improve the performance of the model.\n",
        "\n",
        "2. Defining the model architecture: The code defines a simple Recurrent Neural Network (RNN) model using the Keras API from TensorFlow 2.x.  ** The model consists of a single LSTM layer with 32 units and a dense output layer with 1 unit.**  The input sequences have a window size of 12, and the output is a single prediction for the next time step.\n",
        "\n",
        "3. Creating and training the model: The code creates training and validation datasets using a sliding window approach, where each input sequence consists of 12 time steps and the corresponding output is the next time step. The model is then trained using the fit method, with early stopping and checkpoint callbacks to save the best model based on the validation loss. The model is trained for a maximum of 50 epochs.\n",
        "\n",
        "4. Evaluating the best model: After training the model, the code loads the best model saved by the checkpoint callback and evaluates it on the validation dataset. The validation loss is used to measure the performance of the model.\n",
        "\n",
        "5. Using callbacks to improve model performance: The code uses early stopping and checkpoint callbacks to improve the performance of the model. Early stopping stops training if the validation loss does not improve for a specified number of epochs, which can help to prevent overfitting. Checkpointing saves the best model based on the validation loss, which can be useful if training is interrupted or if the best model needs to be loaded for later use."
      ],
      "metadata": {
        "id": "lBrAdNjMaPXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the sunspot dataset\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "df = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "series = df['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "data_mean = series.mean(axis=0)\n",
        "data_std = series.std(axis=0)\n",
        "series = (series - data_mean) / data_std\n",
        "\n",
        "# Define the window size for input sequences and output predictions\n",
        "window_size = 12\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "\n",
        "# Define a function to create input/output sequences from the time series data\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_size = int(len(series) * 0.8)\n",
        "train_series = series[:train_size]\n",
        "valid_series = series[train_size:]\n",
        "train_dataset = windowed_dataset(train_series, window_size, batch_size, shuffle_buffer_size)\n",
        "valid_dataset = windowed_dataset(valid_series, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(window_size, 1))\n",
        "x = LSTM(32, activation='relu')(inputs)\n",
        "outputs = Dense(1)(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=50, validation_data=valid_dataset, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n",
        "val_loss = best_model.evaluate(valid_dataset)\n",
        "print('Validation loss of best model:', val_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcE3Nn5rdGcq",
        "outputId": "2460c344-2356-4473-e69a-2e5761734e39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "81/81 [==============================] - 5s 28ms/step - loss: 0.3981 - val_loss: 0.1766\n",
            "Epoch 2/50\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1776 - val_loss: 0.1295\n",
            "Epoch 3/50\n",
            "81/81 [==============================] - 2s 27ms/step - loss: 0.1518 - val_loss: 0.1223\n",
            "Epoch 4/50\n",
            "81/81 [==============================] - 3s 43ms/step - loss: 0.1498 - val_loss: 0.1235\n",
            "Epoch 5/50\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1511 - val_loss: 0.1216\n",
            "Epoch 6/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1438 - val_loss: 0.1190\n",
            "Epoch 7/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1460 - val_loss: 0.1198\n",
            "Epoch 8/50\n",
            "81/81 [==============================] - 3s 42ms/step - loss: 0.1458 - val_loss: 0.1182\n",
            "Epoch 9/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1454 - val_loss: 0.1183\n",
            "Epoch 10/50\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1463 - val_loss: 0.1273\n",
            "Epoch 11/50\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1442 - val_loss: 0.1190\n",
            "Epoch 12/50\n",
            "81/81 [==============================] - 4s 51ms/step - loss: 0.1438 - val_loss: 0.1205\n",
            "Epoch 13/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1417 - val_loss: 0.1168\n",
            "Epoch 14/50\n",
            "81/81 [==============================] - 4s 43ms/step - loss: 0.1419 - val_loss: 0.1189\n",
            "Epoch 15/50\n",
            "81/81 [==============================] - 4s 46ms/step - loss: 0.1438 - val_loss: 0.1210\n",
            "Epoch 16/50\n",
            "81/81 [==============================] - 3s 37ms/step - loss: 0.1419 - val_loss: 0.1162\n",
            "Epoch 17/50\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1417 - val_loss: 0.1256\n",
            "Epoch 18/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1431 - val_loss: 0.1179\n",
            "Epoch 19/50\n",
            "81/81 [==============================] - 2s 29ms/step - loss: 0.1412 - val_loss: 0.1177\n",
            "Epoch 20/50\n",
            "81/81 [==============================] - 3s 37ms/step - loss: 0.1413 - val_loss: 0.1159\n",
            "Epoch 21/50\n",
            "81/81 [==============================] - 3s 33ms/step - loss: 0.1431 - val_loss: 0.1170\n",
            "Epoch 22/50\n",
            "81/81 [==============================] - 3s 41ms/step - loss: 0.1405 - val_loss: 0.1272\n",
            "Epoch 23/50\n",
            "81/81 [==============================] - 3s 38ms/step - loss: 0.1419 - val_loss: 0.1173\n",
            "Epoch 24/50\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1427 - val_loss: 0.1168\n",
            "Epoch 25/50\n",
            "81/81 [==============================] - 3s 33ms/step - loss: 0.1410 - val_loss: 0.1172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1159\n",
            "Validation loss of best model: 0.11587566137313843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 재시도\n",
        "* prompt:  modify this code to get val_loss under 0.07 without any error.\n",
        "* responds : \n",
        "1. Increased the number of epochs to 100: In order to achieve a lower validation loss, the number of epochs has been increased from 50 to 100. This will allow the model more time to learn and converge to a better solution.\n",
        "\n",
        "2. Increased the patience of EarlyStopping to 10: The patience parameter of EarlyStopping has been increased from 5 to 10. This means that the training will stop only if the validation loss does not improve for 10 consecutive epochs.\n",
        "\n",
        "3. Reduced the learning rate of the optimizer: The learning rate of the optimizer has been reduced by setting the learning_rate parameter of the Adam optimizer to a smaller value. This can help the optimizer converge to a better solution.\n",
        "\n",
        "4. Added Dropout layer to prevent overfitting: In order to prevent overfitting, a Dropout layer has been added after the LSTM layer.\n",
        "\n",
        "5. Changed the Dense layer activation function to 'linear'"
      ],
      "metadata": {
        "id": "ABWSFN4zes_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the sunspot dataset\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "df = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "series = df['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "data_mean = series.mean(axis=0)\n",
        "data_std = series.std(axis=0)\n",
        "series = (series - data_mean) / data_std\n",
        "\n",
        "# Define the window size for input sequences and output predictions\n",
        "window_size = 12\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "\n",
        "# Define a function to create input/output sequences from the time series data\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_size = int(len(series) * 0.8)\n",
        "train_series = series[:train_size]\n",
        "valid_series = series[train_size:]\n",
        "train_dataset = windowed_dataset(train_series, window_size, batch_size, shuffle_buffer_size)\n",
        "valid_dataset = windowed_dataset(valid_series, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(window_size, 1))\n",
        "x = LSTM(32, activation='relu')(inputs)\n",
        "x = LSTM(32, activation='relu')(inputs)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "outputs = Dense(1)(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n",
        "val_loss = best_model.evaluate(valid_dataset)\n",
        "print('Validation loss of best model:', val_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWxct3fDgMBZ",
        "outputId": "f9f68a55-f570-436f-c2cb-409ce1590fe5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "81/81 [==============================] - 6s 45ms/step - loss: 0.3887 - val_loss: 0.1469\n",
            "Epoch 2/100\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1582 - val_loss: 0.1416\n",
            "Epoch 3/100\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1511 - val_loss: 0.1205\n",
            "Epoch 4/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1480 - val_loss: 0.1239\n",
            "Epoch 5/100\n",
            "81/81 [==============================] - 3s 37ms/step - loss: 0.1459 - val_loss: 0.1198\n",
            "Epoch 6/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1462 - val_loss: 0.1199\n",
            "Epoch 7/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1440 - val_loss: 0.1191\n",
            "Epoch 8/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1441 - val_loss: 0.1198\n",
            "Epoch 9/100\n",
            "81/81 [==============================] - 3s 40ms/step - loss: 0.1429 - val_loss: 0.1176\n",
            "Epoch 10/100\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1426 - val_loss: 0.1173\n",
            "Epoch 11/100\n",
            "81/81 [==============================] - 2s 24ms/step - loss: 0.1432 - val_loss: 0.1177\n",
            "Epoch 12/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1433 - val_loss: 0.1204\n",
            "Epoch 13/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1406 - val_loss: 0.1170\n",
            "Epoch 14/100\n",
            "81/81 [==============================] - 3s 39ms/step - loss: 0.1421 - val_loss: 0.1177\n",
            "Epoch 15/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1412 - val_loss: 0.1202\n",
            "Epoch 16/100\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1411 - val_loss: 0.1166\n",
            "Epoch 17/100\n",
            "81/81 [==============================] - 3s 35ms/step - loss: 0.1423 - val_loss: 0.1164\n",
            "Epoch 18/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1403 - val_loss: 0.1168\n",
            "Epoch 19/100\n",
            "81/81 [==============================] - 2s 26ms/step - loss: 0.1439 - val_loss: 0.1158\n",
            "Epoch 20/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1423 - val_loss: 0.1162\n",
            "Epoch 21/100\n",
            "81/81 [==============================] - 5s 64ms/step - loss: 0.1402 - val_loss: 0.1188\n",
            "Epoch 22/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1396 - val_loss: 0.1152\n",
            "Epoch 23/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1401 - val_loss: 0.1150\n",
            "Epoch 24/100\n",
            "81/81 [==============================] - 3s 36ms/step - loss: 0.1419 - val_loss: 0.1185\n",
            "Epoch 25/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1413 - val_loss: 0.1157\n",
            "Epoch 26/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1395 - val_loss: 0.1172\n",
            "Epoch 27/100\n",
            "81/81 [==============================] - 2s 24ms/step - loss: 0.1403 - val_loss: 0.1202\n",
            "Epoch 28/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1408 - val_loss: 0.1151\n",
            "Epoch 29/100\n",
            "81/81 [==============================] - 3s 34ms/step - loss: 0.1400 - val_loss: 0.1180\n",
            "Epoch 30/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1404 - val_loss: 0.1159\n",
            "Epoch 31/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1383 - val_loss: 0.1187\n",
            "Epoch 32/100\n",
            "81/81 [==============================] - 2s 25ms/step - loss: 0.1389 - val_loss: 0.1164\n",
            "Epoch 33/100\n",
            "81/81 [==============================] - 2s 29ms/step - loss: 0.1393 - val_loss: 0.1215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 1s 9ms/step - loss: 0.1150\n",
            "Validation loss of best model: 0.11498843133449554\n"
          ]
        }
      ]
    }
  ]
}