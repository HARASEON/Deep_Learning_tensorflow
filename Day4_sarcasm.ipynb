{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWjU1B7iVnVfoSuo2rjjBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day4_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto ML\n",
        "* 개념; This includes automating the data preprocessing, feature engineering, model selection, hyperparameter tuning, and model deployment.\n",
        "* point1: 자동화 -> 인공지능 전문가 인력 부족 해결\n",
        "* point2: 접근성 -> 비전문가에게 더 높은 접근성\n",
        "* point3: 효율성 -> 더 빠르고 효율적인 메뉴얼된 머신러닝제공\n",
        "* point4: 맞춤성 -> 구체적인 목적에 적합한 머신러닝 제공\n",
        "* point5: 재생산성 \n",
        "* point6: 최적화"
      ],
      "metadata": {
        "id": "1HBmIeA_4SBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "* 용도 : used for processing sequential data.\n",
        "* 특징 : feedback connections, which allow them to store information about previous inputs and use that information to make decisions about future inputs.\n",
        "* 장점 : RNN can maintain a sort of \"memory\" of the past inputs it has seen and use that information to make decisions about future inputs. This makes RNNs particularly useful for processing sequential data, such as time series or natural language.\n",
        "* 문제점 :  the gradients can quickly become very small or very large, a problem known as ** the vanishing gradient problem.**\n",
        "* 해결방안 : This can make it difficult for the network to learn long-term dependencies. To address this, various types of RNNs have been developed, such as LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), which use more sophisticated architectures to help preserve the gradients over longer sequences.(RNN의 장기 의존성 문제 해결)\n"
      ],
      "metadata": {
        "id": "Uco-oSQ_4djS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarcasm \n",
        "* Goal: val_loss < 0.3626\n",
        "* data length: 26709 ea\n",
        "* Dense layer를 추가로 늘려봤는데 오히려 더 적은 것이 학습률이 뛰어나다는 것을 배웠다!!\n",
        " "
      ],
      "metadata": {
        "id": "1brsXwO1-Wo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na_g_rr14GvG",
        "outputId": "5b7b31e6-223d-4fbd-9f96-37f8888e3b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.7749\n",
            "Epoch 1: val_loss improved from inf to 0.39407, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 40s 50ms/step - loss: 0.4495 - accuracy: 0.7749 - val_loss: 0.3941 - val_accuracy: 0.8152\n",
            "Epoch 2/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3482 - accuracy: 0.8419\n",
            "Epoch 2: val_loss improved from 0.39407 to 0.37324, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 20s 33ms/step - loss: 0.3482 - accuracy: 0.8419 - val_loss: 0.3732 - val_accuracy: 0.8305\n",
            "Epoch 3/100\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.3296 - accuracy: 0.8519\n",
            "Epoch 3: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 21s 34ms/step - loss: 0.3292 - accuracy: 0.8521 - val_loss: 0.3846 - val_accuracy: 0.8256\n",
            "Epoch 4/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.8612\n",
            "Epoch 4: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 33ms/step - loss: 0.3103 - accuracy: 0.8612 - val_loss: 0.3924 - val_accuracy: 0.8267\n",
            "Epoch 5/100\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8712\n",
            "Epoch 5: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.2966 - accuracy: 0.8712 - val_loss: 0.3859 - val_accuracy: 0.8283\n",
            "Epoch 6/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8753\n",
            "Epoch 6: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2833 - accuracy: 0.8753 - val_loss: 0.4094 - val_accuracy: 0.8188\n",
            "Epoch 7/100\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.8820\n",
            "Epoch 7: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2723 - accuracy: 0.8820 - val_loss: 0.3939 - val_accuracy: 0.8284\n",
            "Epoch 8/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.8867\n",
            "Epoch 8: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.2617 - accuracy: 0.8867 - val_loss: 0.3947 - val_accuracy: 0.8277\n",
            "Epoch 9/100\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.8890\n",
            "Epoch 9: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 33ms/step - loss: 0.2558 - accuracy: 0.8890 - val_loss: 0.3842 - val_accuracy: 0.8229\n",
            "Epoch 10/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.8927\n",
            "Epoch 10: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.2483 - accuracy: 0.8927 - val_loss: 0.3985 - val_accuracy: 0.8259\n",
            "Epoch 11/100\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.8979\n",
            "Epoch 11: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2409 - accuracy: 0.8980 - val_loss: 0.4260 - val_accuracy: 0.8258\n",
            "Epoch 12/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9018\n",
            "Epoch 12: val_loss did not improve from 0.37324\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2355 - accuracy: 0.9018 - val_loss: 0.4352 - val_accuracy: 0.8189\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Bidirectional, LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Data loading\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "with open('sarcasm.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Data preprocessing\n",
        "sentences = []\n",
        "labels = []\n",
        "for d in data:\n",
        "  sentences.append(d['headline'])\n",
        "  labels.append(d['is_sarcastic'])\n",
        "\n",
        "training_size = 20000\n",
        "train_sentences = sentences[:training_size]\n",
        "train_labels = labels[:training_size]\n",
        "\n",
        "test_sentences = sentences[training_size:]\n",
        "test_labels = labels[training_size:]\n",
        "\n",
        "# tokenizer\n",
        "vocab_size = 1000  # 25637\n",
        "oov_tok = \"<oov>\"  # oov = out of vacabulary\n",
        "\n",
        "# max_length: 헤드라인에 들어가는 최대 단어수\n",
        "max_length = 120\n",
        "# 잘라낼 문장의 위치\n",
        "trunc_type='post'\n",
        "# 채워줄 문장의 위치 -> 채워주려면 'pre'를 쓴다.\n",
        "padding_type='post'\n",
        "\n",
        "embedding_dim = 16   # word2vec\n",
        "\n",
        "# 단어의 토큰화\n",
        "# oov: out of vocabulary\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# 문장에 대한 토큰화\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 문장을 토큰으로 변경\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "# Test문장에도 토크나이저 적용\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# label 값을 Numpy Array로 변환\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "\n",
        "# model building\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# model fitting\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model monitoring\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "checkpoint_path = 'my_checkpoint.ckpt'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "    save_weights_only=True, \n",
        "    save_best_only=True, \n",
        "    monitor='val_loss',\n",
        "    verbose=1)\n",
        "\n",
        "# model_learning & validation\n",
        "history = model.fit(x_train, y_train, \n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[checkpoint, early_stopping],\n",
        "    epochs=100)\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "model.save(\"sarcasm.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT\n",
        "* prompt:code with sarcasm dataset from 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json' and do the RNN and modify it to get val_loss under 0.3626 success!! 0.3283\n",
        "* respond:\n",
        "1. Loading and preprocessing the dataset: The code loads the sarcasm dataset in JSON format from a URL using the tf.keras.utils.get_file function. Then, it extracts the sentences and labels from the dataset and splits them into training and validation sets. The sentences are tokenized using the Tokenizer class and padded to a fixed length using the pad_sequences function.\n",
        "\n",
        "2. Defining the RNN model: The code defines a sequential model using the Keras API with the following layers: an embedding layer, a dropout layer, a bidirectional LSTM layer, another dropout layer, a dense layer with ReLU activation, another dropout layer, and finally a dense layer with sigmoid activation. The embedding layer learns the meaning of each word in the input sequence and the LSTM layer captures the context of the sequence.\n",
        "\n",
        "3. Compiling the model: The model is compiled with binary cross-entropy loss and Adam optimizer. The accuracy metric is also included to monitor the performance of the model during training.\n",
        "\n",
        "4. Training the model: The model is trained for 30 epochs on the training set with a batch size of 32 using the fit function. The validation set is also provided to monitor the performance of the model on unseen data during training. A checkpoint callback is defined to save the best weights of the model based on the validation loss.\n",
        "\n",
        "5. Evaluating the model: After training, the model is evaluated on the validation set using the evaluate function. The validation loss and accuracy are printed to the console. Finally, the best weights of the model are saved to a .h5 file named \"sarcasm_gpt.h5\"."
      ],
      "metadata": {
        "id": "J84K7wvOEHWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the sarcasm dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
        "response = tf.keras.utils.get_file('sarcasm.json', url)\n",
        "with open(response, 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "# Extract the sentences and labels from the dataset\n",
        "sentences = []\n",
        "labels = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(int(item['is_sarcastic']))\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "training_size = int(len(sentences) * 0.8)\n",
        "training_sentences = sentences[0:training_size]\n",
        "validation_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "validation_labels = labels[training_size:]\n",
        "\n",
        "# Tokenize the sentences and pad them to a fixed length\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=100, padding='post', truncating='post')\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# Define the RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(10000, 16, input_length=100),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint_filepath = 'my_checkpoint.h5'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Train the model # 이 부분 에러: training_labels가가 numpy가 아니라 리스트라 생긴 에러\n",
        "history = model.fit(x=training_padded, y=training_labels, epochs=30, validation_data=(validation_padded, validation_labels),\n",
        "                    callbacks=[checkpoint_callback])\n",
        "\n",
        "# Load the best weights from the checkpoint file\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Save the best weights\n",
        "model.save(\"sarcasm_gpt.h5\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "loss, accuracy = model.evaluate(validation_padded, validation_labels)\n",
        "print(\"Validation loss:\", loss)\n",
        "print(\"Validation accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "8Gl4YOBR1FKH",
        "outputId": "a8c08939-b37f-47b1-bb4a-0061d762726e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b06a980cf7c0>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m history = model.fit(x=training_padded, y=training_labels, epochs=30, validation_data=(validation_padded, validation_labels),\n\u001b[0m\u001b[1;32m     60\u001b[0m                     callbacks=[checkpoint_callback])\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the sarcasm dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
        "response = tf.keras.utils.get_file('sarcasm.json', url)\n",
        "with open(response, 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "# Extract the sentences and labels from the dataset\n",
        "sentences = []\n",
        "labels = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(int(item['is_sarcastic']))\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "training_size = int(len(sentences) * 0.8)\n",
        "training_sentences = sentences[0:training_size]\n",
        "validation_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "validation_labels = labels[training_size:]\n",
        "\n",
        "# Tokenize the sentences and pad them to a fixed length\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=100, padding='post', truncating='post')\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# Define the RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(10000, 16, input_length=100),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint_filepath = 'my_checkpoint.h5'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x=training_padded, y=np.array(training_labels), epochs=50, validation_data=(validation_padded, np.array(validation_labels)),\n",
        "                    callbacks=[checkpoint_callback])\n",
        "\n",
        "# Load the best weights from the checkpoint file\n",
        "# Save the best weights\n",
        "model.load_weights(checkpoint_filepath)\n",
        "model.save(\"sarcasm_gpt.h5\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "loss, accuracy = model.evaluate(validation_padded, np.array(validation_labels))\n",
        "print(\"Validation loss:\", loss)\n",
        "print(\"Validation accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFl9AD2X1XB-",
        "outputId": "8a109b79-61da-4144-a7c2-166a284c012b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "668/668 [==============================] - 29s 37ms/step - loss: 0.5406 - accuracy: 0.7195 - val_loss: 0.3613 - val_accuracy: 0.8506\n",
            "Epoch 2/30\n",
            "668/668 [==============================] - 12s 18ms/step - loss: 0.3474 - accuracy: 0.8669 - val_loss: 0.3283 - val_accuracy: 0.8517\n",
            "Epoch 3/30\n",
            "668/668 [==============================] - 11s 16ms/step - loss: 0.2841 - accuracy: 0.8954 - val_loss: 0.3391 - val_accuracy: 0.8560\n",
            "Epoch 4/30\n",
            "668/668 [==============================] - 12s 19ms/step - loss: 0.2560 - accuracy: 0.9078 - val_loss: 0.3665 - val_accuracy: 0.8525\n",
            "Epoch 5/30\n",
            "668/668 [==============================] - 13s 19ms/step - loss: 0.2313 - accuracy: 0.9159 - val_loss: 0.3764 - val_accuracy: 0.8512\n",
            "Epoch 6/30\n",
            "668/668 [==============================] - 11s 16ms/step - loss: 0.2101 - accuracy: 0.9239 - val_loss: 0.4150 - val_accuracy: 0.8487\n",
            "Epoch 7/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1992 - accuracy: 0.9276 - val_loss: 0.4340 - val_accuracy: 0.8398\n",
            "Epoch 8/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1832 - accuracy: 0.9328 - val_loss: 0.4647 - val_accuracy: 0.8459\n",
            "Epoch 9/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1754 - accuracy: 0.9364 - val_loss: 0.4787 - val_accuracy: 0.8416\n",
            "Epoch 10/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1639 - accuracy: 0.9388 - val_loss: 0.4864 - val_accuracy: 0.8399\n",
            "Epoch 11/30\n",
            "668/668 [==============================] - 8s 12ms/step - loss: 0.1532 - accuracy: 0.9458 - val_loss: 0.5366 - val_accuracy: 0.8448\n",
            "Epoch 12/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1480 - accuracy: 0.9476 - val_loss: 0.5675 - val_accuracy: 0.8439\n",
            "Epoch 13/30\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.1466 - accuracy: 0.9482 - val_loss: 0.5485 - val_accuracy: 0.8454\n",
            "Epoch 14/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1329 - accuracy: 0.9521 - val_loss: 0.6321 - val_accuracy: 0.8390\n",
            "Epoch 15/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1349 - accuracy: 0.9509 - val_loss: 0.5785 - val_accuracy: 0.8443\n",
            "Epoch 16/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1340 - accuracy: 0.9512 - val_loss: 0.5857 - val_accuracy: 0.8405\n",
            "Epoch 17/30\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.1267 - accuracy: 0.9552 - val_loss: 0.6428 - val_accuracy: 0.8428\n",
            "Epoch 18/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1252 - accuracy: 0.9568 - val_loss: 0.6316 - val_accuracy: 0.8411\n",
            "Epoch 19/30\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.1242 - accuracy: 0.9547 - val_loss: 0.7245 - val_accuracy: 0.8435\n",
            "Epoch 20/30\n",
            "668/668 [==============================] - 11s 16ms/step - loss: 0.1143 - accuracy: 0.9585 - val_loss: 0.7173 - val_accuracy: 0.8435\n",
            "Epoch 21/30\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.1146 - accuracy: 0.9573 - val_loss: 0.7274 - val_accuracy: 0.8418\n",
            "Epoch 22/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1120 - accuracy: 0.9598 - val_loss: 0.7495 - val_accuracy: 0.8416\n",
            "Epoch 23/30\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.1098 - accuracy: 0.9598 - val_loss: 0.7680 - val_accuracy: 0.8420\n",
            "Epoch 24/30\n",
            "668/668 [==============================] - 8s 12ms/step - loss: 0.1115 - accuracy: 0.9608 - val_loss: 0.6868 - val_accuracy: 0.8456\n",
            "Epoch 25/30\n",
            "668/668 [==============================] - 11s 16ms/step - loss: 0.1036 - accuracy: 0.9627 - val_loss: 0.7656 - val_accuracy: 0.8388\n",
            "Epoch 26/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.0989 - accuracy: 0.9644 - val_loss: 0.7725 - val_accuracy: 0.8463\n",
            "Epoch 27/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1049 - accuracy: 0.9623 - val_loss: 0.7922 - val_accuracy: 0.8428\n",
            "Epoch 28/30\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.1032 - accuracy: 0.9627 - val_loss: 0.7499 - val_accuracy: 0.8418\n",
            "Epoch 29/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.1027 - accuracy: 0.9618 - val_loss: 0.7857 - val_accuracy: 0.8377\n",
            "Epoch 30/30\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.0964 - accuracy: 0.9654 - val_loss: 0.9207 - val_accuracy: 0.8398\n",
            "167/167 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8517\n",
            "Validation loss: 0.3283102512359619\n",
            "Validation accuracy: 0.8517408967018127\n"
          ]
        }
      ]
    }
  ]
}