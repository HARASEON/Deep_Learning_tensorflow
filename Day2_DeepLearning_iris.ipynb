{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMas333Te7wvuHNme1/3pJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day2_DeepLearning_iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iris prediction with tensorflow\n",
        "* Iris data 특징\n",
        "* 붓꽃의 3가지 종류를 기록한 데이터\n",
        "* label: 붓꽃의 종류인 traget -> 0,1,2\n",
        "* 150개 데이터\n"
      ],
      "metadata": {
        "id": "_3mJKzCnV6YW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rceimMCeUIsJ",
        "outputId": "8fcfa63d-92c9-4482-c275-ec006effbe4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_21 (Dense)            (None, 6)                 30        \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 5)                 35        \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 3)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83\n",
            "Trainable params: 83\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "[0.012088 0.504675 0.483237]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float_kind': lambda x: \"{0:0.6f}\".format(x)})\n",
        "\n",
        "\n",
        "# load dataset\n",
        "train_ds = tfds.load('iris', split = 'train[:80%]')\n",
        "test_ds = tfds.load('iris', split= 'train[-20%:]')\n",
        "\n",
        "# data preprocessing\n",
        "# label : 3ea\n",
        "def preprocess(features):\n",
        "    f,l = features['features'], tf.one_hot(features['label'], 3)\n",
        "    return f,l\n",
        "\n",
        "train_data = train_ds.map(preprocess).batch(10)\n",
        "valid_data = test_ds.map(preprocess).batch(10)\n",
        "\n",
        "# design model\n",
        "model = Sequential([\n",
        "    Dense(6, activation=\"relu\"),\n",
        "    Dense(5, activation=\"relu\"),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "  \n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc']) \n",
        "\n",
        "# model fitting\n",
        "model.fit(train_data,\n",
        "    validation_data=(valid_data),\n",
        "    verbose=0,\n",
        "    epochs=50)\n",
        "\n",
        "model.save(\"iris.h5\")\n",
        "model.summary()\n",
        "\n",
        "# model Use\n",
        "pred_test = model.predict(valid_data)\n",
        "print(pred_test[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Iris dataset이 CNN을 사용할 수 없는 이유:\n",
        "image dataset이 아니라서 CNN은 이미지 데이터셋 분류를 위해 설계된 알고리즘이다."
      ],
      "metadata": {
        "id": "2iIB0qiodBJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat GPT with iris dataset\n",
        "# No CNN\n",
        "* Goal:val_lose <= 0.13\n",
        "* Prompt:please, code efficient, fast and best performing program with minimum val loss value using python tensorflow\n",
        "from iris dataset from tensorflow_datasets\n",
        "to perform classification\n",
        "use only dense layers \n",
        "use dropout\n",
        "use batch normalization\n",
        "use checkpoint with save best only, and save weight only \n",
        "use early stopping with patient 10\n",
        "Use reduce lr on plateau\n",
        "set verbose = 1\n",
        "do not use data augmentation\n",
        "and other method to make val loss value lower than 0.13.\n",
        "\n",
        "* Code detail:\n",
        "This code loads the iris dataset using TensorFlow Datasets, defines a simple fully connected neural network with batch normalization and dropout, compiles the model with an Adam optimizer and sparse categorical cross-entropy loss function, and trains the model using early stopping, learning rate reduction on plateau and model checkpointing to save the best model. Finally, it evaluates the best model on the validation set and prints the validation loss and accuracy.\n",
        "\n",
        "**결과: 에러가 떴다!\n",
        "KeyError: 'validation'**"
      ],
      "metadata": {
        "id": "3OB7AebZct72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load the iris dataset from tensorflow_datasets\n",
        "data, info = tfds.load(\"iris\", with_info=True, as_supervised=True)\n",
        "train_data = data['train']\n",
        "val_data = data['validation']\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', \n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         monitor='val_loss')\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
        "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                                          patience=5, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data.batch(32), \n",
        "                    epochs=100, \n",
        "                    validation_data=val_data.batch(32),\n",
        "                    callbacks=[checkpoint_callback, early_stop_callback, reduce_lr_callback],\n",
        "                    verbose=1)\n",
        "\n",
        "# Load the best saved model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(val_data.batch(32), verbose=0)\n",
        "\n",
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "JBQQRyAwY4zK",
        "outputId": "144dd6f8-2e2d-4ca2-b00e-0ea0581b7760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ec0016d6fd5e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iris\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define the model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'validation'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**따라서, 다시 prompt에 debug하라는 명령을 내렸다**\n",
        "\n",
        "chatGPT: Apologies for my mistake in the previous code. The data dictionary returned by tfds.load(\"iris\", with_info=True, as_supervised=True) does not contain a validation key.\n",
        "\n",
        "** ChatGPT는 이름만 바꿨다.**\n",
        "\n"
      ],
      "metadata": {
        "id": "KcmzrRAsfrOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the iris dataset from tensorflow_datasets\n",
        "data, info = tfds.load(\"iris\", with_info=True, as_supervised=True)\n",
        "train_data = data['train']\n",
        "val_data = data['test']\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', \n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         monitor='val_loss')\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
        "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                                          patience=5, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data.batch(32), \n",
        "                    epochs=100, \n",
        "                    validation_data=val_data.batch(32),\n",
        "                    callbacks=[checkpoint_callback, early_stop_callback, reduce_lr_callback],\n",
        "                    verbose=1)\n",
        "\n",
        "# Load the best saved model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(val_data.batch(32), verbose=0)\n",
        "\n",
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "TU9NQR1cgCKp",
        "outputId": "a863d65b-0814-4fc7-aab1-fbae5310160d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a06bbe2e8b31>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iris\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define the model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'test'"
          ]
        }
      ]
    }
  ]
}