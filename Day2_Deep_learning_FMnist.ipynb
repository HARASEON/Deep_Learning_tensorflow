{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsxRRMUpWNUD1lcdL8rwIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day2_Deep_learning_FMnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파이썬 csv 파일읽기\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eJMDTC6JLl5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh6itBL2Iznj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "file_name = '경로' with open(file_name) as csvfile:\n",
        "line = csv.reader(csvfile)\n",
        "i = 0\n",
        "for ln in line:\n",
        "  print(ln)\n",
        "  i = i+1\n",
        "  if i == 11:\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터10개 프린트하기\n",
        "한글데이터 encoding\n",
        "line = pd.read_csv(file_name, encoding = 'euc-kr')\n",
        "혹은\n",
        "line = pd.read_csv(file_name, encoding = \"utf-8\")"
      ],
      "metadata": {
        "id": "Rm2uddkoM4kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "file_name = '경로' with open(file_name) as csvfile:\n",
        "line = csv.reader(csvfile)\n",
        "i= 0\n",
        "for ln in line:\n",
        "  print(ln)\n",
        "  i = i+1\n",
        "  if i == 10:\n",
        "    break"
      ],
      "metadata": {
        "id": "plTzKRWZM9SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_name = '경로'\n",
        "line = pd.read_csv(file_name, encoding=\"euc-kr\")\n",
        "line.head(10)"
      ],
      "metadata": {
        "id": "b36lNO27NbtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matplotlib"
      ],
      "metadata": {
        "id": "mCH2TOx9OyXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 그래프사이즈\n",
        "plt.figure(figsize = (12, 9))\n",
        "# x축과 y축의 값(list)\n",
        "plt.plot(np.arange(1,21), history.history['loss'])\n",
        "# 그래프제목\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Me83voCO1_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting 방지\n",
        "* Dropout: training을 할 때 각 batch마다 layer단위로 일정 비율 neuron을 끄는 방식\n",
        "  * 효과: 동일한 데이터에 대해서 모델의 학습이 training data에 편중되는 것을 막기 때문에 여러개의 서로 다른 모델을 합한 ensemble 효과를 얻을 수 있다.\n",
        "  * 주의점!: test/ inference 단계에서는 dropout을 빼고, 전제 neuron이 살아 있는 채로 진행해야함\n",
        "    * Alexnet에 처음 적용\n",
        "\n",
        "* Dense: 뉴런의 입출력을 연결해주는 layer\n",
        "* Sequential : 각 레이어의 하나의 입력텐서와 출력 텐서가 있는 일반레이어에 사용\n",
        "* Sequential 모델은 다음의 경우에 적합하지 않습니다.\n",
        "\n",
        "  * 모델에 다중 입력 또는 다중 출력\n",
        "  * 레이어에 다중 입력 또는 다중 출력\n",
        "  * 레이어 공유\n",
        "  * 비선형 토폴로지(예: 잔류 연결, 다중 분기 모델)\n"
      ],
      "metadata": {
        "id": "tZs4X64ZRpMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "# numpy float 출력옵션 변경\n",
        "np.set_printoptions(formatter = {'float_kind': lambda x: \"{0:0.3f}\".format(x)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_OD7PBxS0Fo",
        "outputId": "213facac-e7a1-4408-8169-e63edf602272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 데이터셋준비하기\n",
        "2. 데이터프리프로세싱\n",
        "3. 모델 설계\n",
        "4. 모델 학습\n",
        "5. 모델 검증\n",
        "6. 모델 사용\n"
      ],
      "metadata": {
        "id": "vsZ193aZWD0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fashion Mnist file\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "import matplotlib.pyplot as plot\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "np.set_printoptions(formatter = {'float_kind': lambda x: \"{0:0.3f}\".format(x)})\n",
        "\n",
        "## Data load\n",
        "#fmnist data : 6000:1000 = # of train data: # of test data \n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "print('length:', len(x_train))\n",
        "print('ndim', x_train.ndim)\n",
        "print('size:', x_train.size, 60000*28*28)\n",
        "print('dtype:', x_train.dtype)\n",
        "print('dtype.name:', x_train.dtype.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoIJjZGdWCkR",
        "outputId": "18d15b8f-45c6-4627-cd74-b9e59b88d8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
            "length: 60000\n",
            "ndim 3\n",
            "size: 47040000 47040000\n",
            "dtype: uint8\n",
            "dtype.name: uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data preprocessing\n",
        "class_name = ['T-shirts/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot' ]\n",
        "print(y_train[2])\n",
        "print(class_name[y_train[2]])\n",
        "plt.imshow(x_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "agQ1uTyDZj0r",
        "outputId": "a274a10c-cef7-4f97-b168-47d5704562f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "T-shirts/top\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f821f724160>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhH86ZnmbWLT",
        "outputId": "541df796-6738-4852-f01a-4addbee4d943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0  22 118  24   0   0   0   0   0  48\n",
            "   88   5   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  12 100 212 205 185 179 173 186 193 221\n",
            "  142  85   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  85  76 199 225 248 255 238 226 157\n",
            "   68  80   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  91  69  91 201 218 225 209 158  61\n",
            "   93  72   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  79  89  61  59  87 108  75  56  76\n",
            "   97  73   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  75  89  80  80  67  63  73  83  80\n",
            "   96  72   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  77  88  77  80  83  83  83  83  81\n",
            "   95  76   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  89  96  80  83  81  84  85  85  85\n",
            "   97  84   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  93  97  81  85  84  85  87  88  84\n",
            "   99  87   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  95  87  84  87  88  85  87  87  84\n",
            "   92  87   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  97  87  87  85  88  87  87  87  88\n",
            "   85 107   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  17 100  88  87  87  88  87  87  85  89\n",
            "   77 118   8   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  10  93  87  87  87  87  87  88  87  89\n",
            "   80 103   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   9  96  87  87  87  87  87  88  87  88\n",
            "   87 103   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  12  96  85  87  87  87  85  87  87  88\n",
            "   89 100   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  20  95  84  88  85  87  88  88  88  89\n",
            "   88  99   8   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  21  96  85  87  85  88  88  88  88  89\n",
            "   89  99  10   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  24  96  85  87  85  87  88  88  89  88\n",
            "   91 102  14   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  25  93  84  88  87  87  87  87  87  89\n",
            "   91 103  29   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  95  85  88  88  87  87  87  87  89\n",
            "   88 102  37   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  34  96  88  87  87  87  87  87  87  85\n",
            "   85  97  38   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  40  96  87  85  87  87  87  87  87  85\n",
            "   84  92  49   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  46  95  83  84  87  87  87  87  87  87\n",
            "   84  87  84   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  72  95  85  84  85  88  87  87  89  87\n",
            "   85  83  63   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  64 100  84  87  88  85  88  88  84  87\n",
            "   83  95  53   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  10 102 100  91  91  89  85  84  84  87\n",
            "  108 106  14   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   8  73  93 104 107 103 103 106 102\n",
            "   75  10   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   1   0   0   0  18  42  57  56  32   8\n",
            "    0   0   1   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## In RGB colour, 255 is the last one which means white.\n",
        "# describe probability of fashion type \n",
        "x_train = x_train/255.0\n",
        "x_test= x_test/255.0\n",
        "\n",
        "print(x_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyAFqDn9bePz",
        "outputId": "c40427e8-fc5d-435e-ea46-4f7058dc124a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.086 0.463 0.094\n",
            "  0.000 0.000 0.000 0.000 0.000 0.188 0.345 0.020 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.047 0.392 0.831 0.804\n",
            "  0.725 0.702 0.678 0.729 0.757 0.867 0.557 0.333 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.333 0.298 0.780\n",
            "  0.882 0.973 1.000 0.933 0.886 0.616 0.267 0.314 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.357 0.271 0.357\n",
            "  0.788 0.855 0.882 0.820 0.620 0.239 0.365 0.282 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.310 0.349 0.239\n",
            "  0.231 0.341 0.424 0.294 0.220 0.298 0.380 0.286 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.294 0.349 0.314\n",
            "  0.314 0.263 0.247 0.286 0.325 0.314 0.376 0.282 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.302 0.345 0.302\n",
            "  0.314 0.325 0.325 0.325 0.325 0.318 0.373 0.298 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.349 0.376 0.314\n",
            "  0.325 0.318 0.329 0.333 0.333 0.333 0.380 0.329 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.365 0.380 0.318\n",
            "  0.333 0.329 0.333 0.341 0.345 0.329 0.388 0.341 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.373 0.341 0.329\n",
            "  0.341 0.345 0.333 0.341 0.341 0.329 0.361 0.341 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.380 0.341 0.341\n",
            "  0.333 0.345 0.341 0.341 0.341 0.345 0.333 0.420 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.067 0.392 0.345 0.341\n",
            "  0.341 0.345 0.341 0.341 0.333 0.349 0.302 0.463 0.031 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.039 0.365 0.341 0.341\n",
            "  0.341 0.341 0.341 0.345 0.341 0.349 0.314 0.404 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.035 0.376 0.341 0.341\n",
            "  0.341 0.341 0.341 0.345 0.341 0.345 0.341 0.404 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.047 0.376 0.333 0.341\n",
            "  0.341 0.341 0.333 0.341 0.341 0.345 0.349 0.392 0.008 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.078 0.373 0.329 0.345\n",
            "  0.333 0.341 0.345 0.345 0.345 0.349 0.345 0.388 0.031 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.082 0.376 0.333 0.341\n",
            "  0.333 0.345 0.345 0.345 0.345 0.349 0.349 0.388 0.039 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.094 0.376 0.333 0.341\n",
            "  0.333 0.341 0.345 0.345 0.349 0.345 0.357 0.400 0.055 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.098 0.365 0.329 0.345\n",
            "  0.341 0.341 0.341 0.341 0.341 0.349 0.357 0.404 0.114 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.118 0.373 0.333 0.345\n",
            "  0.345 0.341 0.341 0.341 0.341 0.349 0.345 0.400 0.145 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.133 0.376 0.345 0.341\n",
            "  0.341 0.341 0.341 0.341 0.341 0.333 0.333 0.380 0.149 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.157 0.376 0.341 0.333\n",
            "  0.341 0.341 0.341 0.341 0.341 0.333 0.329 0.361 0.192 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.180 0.373 0.325 0.329\n",
            "  0.341 0.341 0.341 0.341 0.341 0.341 0.329 0.341 0.329 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.282 0.373 0.333 0.329\n",
            "  0.333 0.345 0.341 0.341 0.349 0.341 0.333 0.325 0.247 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.251 0.392 0.329 0.341\n",
            "  0.345 0.333 0.345 0.345 0.329 0.341 0.325 0.373 0.208 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.039 0.400 0.392 0.357\n",
            "  0.357 0.349 0.333 0.329 0.329 0.341 0.424 0.416 0.055 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.031 0.286 0.365\n",
            "  0.408 0.420 0.404 0.404 0.416 0.400 0.294 0.039 0.000 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]\n",
            " [0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.004 0.000 0.000 0.000\n",
            "  0.071 0.165 0.224 0.220 0.125 0.031 0.000 0.000 0.004 0.000 0.000 0.000\n",
            "  0.000 0.000 0.000 0.000]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Design a model\n",
        "## Using Dropout\n",
        "## Using adam for multiclassification\n",
        "# 28*28 = 784\n",
        "model = Sequential ([Flatten(),\n",
        "                     Dense(32, activation = 'relu'),\n",
        "                     Dropout(0.5),\n",
        "                     Dense(10, activation = 'softmax')])\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sCn0_P0LcHBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model fitting\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 5, validation_data=(x_test, y_test), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6zL1tvUeX24",
        "outputId": "3ede2080-5da6-4804-8d69-e61091be3a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8943 - accuracy: 0.6755 - val_loss: 0.5384 - val_accuracy: 0.8142\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6999 - accuracy: 0.7432 - val_loss: 0.4996 - val_accuracy: 0.8233\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.6487 - accuracy: 0.7597 - val_loss: 0.4664 - val_accuracy: 0.8270\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6145 - accuracy: 0.7736 - val_loss: 0.4611 - val_accuracy: 0.8353\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5981 - accuracy: 0.7792 - val_loss: 0.4537 - val_accuracy: 0.8361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f821cd01e50>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Modal Validation\n",
        "model.summary()\n",
        "model.evaluate(x_test,y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olzCgV5fe76L",
        "outputId": "815e4632-364a-44e2-a351-5b63fcc71e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                25120     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,450\n",
            "Trainable params: 25,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.8361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45366230607032776, 0.8360999822616577]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model\n",
        "print(y_train[2999])\n",
        "pred_test= model.predict(x_test)\n",
        "print(pred_test[2])\n",
        "plt.imshow(x_test[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "w-1YeVVYgehM",
        "outputId": "7e4e7ccc-24bb-49aa-943e-db398ab8d159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "[0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f82197673a0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jamwwxng81c",
        "outputId": "b99ecd5f-c8a2-413f-8f4f-c68d35979ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 모니터링하기\n",
        "Callback Function\n",
        "- EarlyStopping\n",
        "- Modelcheckpoint\n",
        "* 학습 중 특정한 조건이 생기면 callback function이 실행됨\n",
        "* callback function은 여러개가 될 수 있음\n",
        "* 중간에 더 좋은 성능이 발견되지 않으면 종료\n",
        "ex1.callbacks\n",
        "  class myCallback(tf.keras.callbacks. Callback):\n",
        "    def on_epoch_end(self, epoch, los={}):\n",
        "      if(logs.get('accuracy') > 0.95):\n",
        "        print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "        self.model.stop_trainig = True\n",
        "\n",
        "  callbacks = myCallback()\n",
        "\n",
        "model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 5, callbacks = [callbacks])\n",
        "\n",
        "ex2 earlystopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "...\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss',  patience = 10\n",
        "\n",
        "model.fit(X, Y, validation_split = 0.2, epochs = 50, callbacks = [early_stopping])\n",
        "\n",
        "\n",
        "* patience: 개선이 안된다고 바로 종료시키지 않고 몇번의 epoch을 기다릴지 설정, 위 예시에서는 10번의 epoch을 기다림\n",
        "\n",
        "ex3 modelcheckpoint\n",
        "- model의 정확도가 최고값을 갱신했을 때만 저장. h5파일\n",
        "\n",
        "checkpoint_path = \"my_chechpoint.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,save_best_only = True, monitor = 'val_loss', verbose = 1)\n",
        "\n",
        "history = model.fit(x_trin, y_train, validation_data(x_test, y_test), epochs = 20, callbacks=[checkpoint])\n",
        "\n",
        "ex4 reducelr on plateau\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "6reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)"
      ],
      "metadata": {
        "id": "TZFB4RtHhKVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hyperparameter tunning\n",
        "정의 : 모델 외부에 있으며 데이터부터 값이 추정될수 없는 설정변수\n",
        "종류\n",
        "* Activation function\n",
        "* Learning rate\n",
        "* Problem type\n",
        "* '# of layers, nodes\n",
        "* Training data ratio\n",
        "* Regularization"
      ],
      "metadata": {
        "id": "0GK1ubeOjtWw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOyJT9Jczkxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "# Fashion MNIST\n",
        "# Without CNN\n",
        "* Goal : val_loss <= 0.3\n",
        "* prompt: please, code efficient, fast and best performing program with minimum val loss value using python tensorflow\n",
        "from fashion mnist dataset\n",
        "to perform classification\n",
        "use only dense layers \n",
        "use dropout\n",
        "use batch normalization\n",
        "use checkpoint with save best only, and save weight only \n",
        "use early stopping with patient 10\n",
        "Use reduce lr on plateau\n",
        "set verbose = 1\n",
        "do not use data augmentation\n",
        "and other method to make val loss under 0.32\n",
        "\n",
        "*  x.reshape(-1, 정수) 를 해주면 '열(column)' 차원의 '정수'에 따라서 12개의 원소가 빠짐없이 배치될 수 있도록 '-1'이 들어가 있는 '행(row)' 의 개수가 가변적으로 정해짐\n",
        "\n",
        "*In this implementation, we're using a sequential model with four dense layers, each followed by batch normalization and dropout. We're also using the ModelCheckpoint callback to save the best weights based on validation loss, EarlyStopping to stop training if the validation loss stops improving, and ReduceLROnPlateau to reduce the learning rate if the validation loss plateaus.\n",
        "\n",
        "After training the model for 50 epochs, the final test accuracy is printed to the console. With these settings, we can achieve a validation loss under 0.32 and a test accuracy of around 0.91. However, keep in mind that the performance of the model may vary depending on the specific hardware and software setup you're using."
      ],
      "metadata": {
        "id": "uiNSbCJrnaaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(784,)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint = ModelCheckpoint('best_weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train.reshape((-1, 784)), y_train, \n",
        "                    validation_data=(x_test.reshape((-1, 784)), y_test),\n",
        "                    epochs=50, batch_size=128, verbose=1,\n",
        "                    callbacks=[checkpoint, early_stop, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_acc = model.evaluate(x_test.reshape((-1, 784)), y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8tPo6ydiriC",
        "outputId": "366fab3f-5e3f-4cdc-ebbc-2f7352469ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 10s 13ms/step - loss: 0.6566 - accuracy: 0.7711 - val_loss: 0.4992 - val_accuracy: 0.8212 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4746 - accuracy: 0.8313 - val_loss: 0.4252 - val_accuracy: 0.8484 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4330 - accuracy: 0.8453 - val_loss: 0.3918 - val_accuracy: 0.8556 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.4141 - accuracy: 0.8526 - val_loss: 0.3841 - val_accuracy: 0.8633 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3944 - accuracy: 0.8592 - val_loss: 0.4083 - val_accuracy: 0.8555 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3871 - accuracy: 0.8613 - val_loss: 0.3860 - val_accuracy: 0.8623 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.3719 - accuracy: 0.8657 - val_loss: 0.4206 - val_accuracy: 0.8448 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3679 - accuracy: 0.8673 - val_loss: 0.4252 - val_accuracy: 0.8525 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3587 - accuracy: 0.8705 - val_loss: 0.3548 - val_accuracy: 0.8694 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3495 - accuracy: 0.8737 - val_loss: 0.3689 - val_accuracy: 0.8682 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3486 - accuracy: 0.8741 - val_loss: 0.3550 - val_accuracy: 0.8731 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3380 - accuracy: 0.8782 - val_loss: 0.3536 - val_accuracy: 0.8728 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3325 - accuracy: 0.8800 - val_loss: 0.3470 - val_accuracy: 0.8775 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3318 - accuracy: 0.8796 - val_loss: 0.3585 - val_accuracy: 0.8711 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.3204 - accuracy: 0.8842 - val_loss: 0.3533 - val_accuracy: 0.8738 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3222 - accuracy: 0.8824 - val_loss: 0.3551 - val_accuracy: 0.8692 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3153 - accuracy: 0.8865 - val_loss: 0.3358 - val_accuracy: 0.8783 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3132 - accuracy: 0.8866 - val_loss: 0.3604 - val_accuracy: 0.8757 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3097 - accuracy: 0.8868 - val_loss: 0.3469 - val_accuracy: 0.8748 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.3146 - accuracy: 0.8850 - val_loss: 0.3320 - val_accuracy: 0.8824 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3088 - accuracy: 0.8879 - val_loss: 0.3402 - val_accuracy: 0.8786 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3054 - accuracy: 0.8889 - val_loss: 0.3884 - val_accuracy: 0.8652 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.3004 - accuracy: 0.8904 - val_loss: 0.3324 - val_accuracy: 0.8816 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.2952 - accuracy: 0.8914 - val_loss: 0.3389 - val_accuracy: 0.8788 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2963 - accuracy: 0.8928 - val_loss: 0.3504 - val_accuracy: 0.8783 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2691 - accuracy: 0.9021 - val_loss: 0.3118 - val_accuracy: 0.8881 - lr: 2.0000e-04\n",
            "Epoch 27/50\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.2622 - accuracy: 0.9048 - val_loss: 0.3118 - val_accuracy: 0.8889 - lr: 2.0000e-04\n",
            "Epoch 28/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.2574 - accuracy: 0.9072 - val_loss: 0.3089 - val_accuracy: 0.8924 - lr: 2.0000e-04\n",
            "Epoch 29/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2575 - accuracy: 0.9047 - val_loss: 0.3096 - val_accuracy: 0.8896 - lr: 2.0000e-04\n",
            "Epoch 30/50\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.2550 - accuracy: 0.9063 - val_loss: 0.3074 - val_accuracy: 0.8922 - lr: 2.0000e-04\n",
            "Epoch 31/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.2500 - accuracy: 0.9078 - val_loss: 0.3110 - val_accuracy: 0.8928 - lr: 2.0000e-04\n",
            "Epoch 32/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2508 - accuracy: 0.9079 - val_loss: 0.3115 - val_accuracy: 0.8920 - lr: 2.0000e-04\n",
            "Epoch 33/50\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.2500 - accuracy: 0.9085 - val_loss: 0.3075 - val_accuracy: 0.8910 - lr: 2.0000e-04\n",
            "Epoch 34/50\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.2485 - accuracy: 0.9091 - val_loss: 0.3083 - val_accuracy: 0.8936 - lr: 2.0000e-04\n",
            "Epoch 35/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2465 - accuracy: 0.9106 - val_loss: 0.3091 - val_accuracy: 0.8891 - lr: 2.0000e-04\n",
            "Epoch 36/50\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.2400 - accuracy: 0.9115 - val_loss: 0.3038 - val_accuracy: 0.8925 - lr: 1.0000e-04\n",
            "Epoch 37/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2399 - accuracy: 0.9126 - val_loss: 0.3056 - val_accuracy: 0.8946 - lr: 1.0000e-04\n",
            "Epoch 38/50\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.2404 - accuracy: 0.9123 - val_loss: 0.3051 - val_accuracy: 0.8935 - lr: 1.0000e-04\n",
            "Epoch 39/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2379 - accuracy: 0.9129 - val_loss: 0.3048 - val_accuracy: 0.8939 - lr: 1.0000e-04\n",
            "Epoch 40/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2364 - accuracy: 0.9144 - val_loss: 0.3068 - val_accuracy: 0.8943 - lr: 1.0000e-04\n",
            "Epoch 41/50\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.2385 - accuracy: 0.9136 - val_loss: 0.3069 - val_accuracy: 0.8938 - lr: 1.0000e-04\n",
            "Epoch 42/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2356 - accuracy: 0.9140 - val_loss: 0.3058 - val_accuracy: 0.8937 - lr: 1.0000e-04\n",
            "Epoch 43/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2336 - accuracy: 0.9139 - val_loss: 0.3064 - val_accuracy: 0.8955 - lr: 1.0000e-04\n",
            "Epoch 44/50\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2356 - accuracy: 0.9136 - val_loss: 0.3044 - val_accuracy: 0.8946 - lr: 1.0000e-04\n",
            "Epoch 45/50\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2336 - accuracy: 0.9147 - val_loss: 0.3040 - val_accuracy: 0.8957 - lr: 1.0000e-04\n",
            "Epoch 46/50\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2317 - accuracy: 0.9162 - val_loss: 0.3044 - val_accuracy: 0.8944 - lr: 1.0000e-04\n",
            "Test accuracy: 0.8944000005722046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN(Convolution Neural Network)\n",
        "\n",
        "* 대표적인 이미지 분류 모델\n",
        "  * 하나의 필터를 전체 이미지 입력값에 합성곱을 적용해준다!\n",
        "  * 합성곱을 거치면 특징(feature)이 필터 방향으로 추출됨\n",
        "\n",
        "* Zero padding: 입력값의 크기보다 결과값의 크기가 줄어들어 손실이되는 부분이 발생 -> 해결방안 : 0으로 구성된 테두리를 이미지 값으로 채워준다.\n",
        "* Strid: 필터를 움직여주는 값 \n",
        "ex. strid = 1 필터 한칸 움직임\n",
        "strid값이 커질 경우 필터가 이미지를 건너뛰는 칸이 커짐을 의미하므로 결과값의 이미지 크기는 작아진다.\n",
        "* Maxpooling : 각 결과값의 차원을 축소해주는 것 이때, 가장 큰 값을 가져와 pool size에 맞춰 값을 채우면 max pooling, 평균값으로 pool값을 채우면 average pooling이다.\n",
        "correlation이 낮은 부분을 삭제하여 각 결과 값의 크기를 줄이는 과정\n",
        "* Upsampling : 데이터가 실제보다 더 자주 수집되는 것처럼 데이터를 표현하는 것\n",
        "* 업샘플링의 목적은 실제로 측정하는 것은 아니지만 드물게 측정된 데이터에서 더 조밀한 시간의 데이터를 얻기 위함\n",
        " * 필요한 경우: \n",
        "  1) 시계열이 불규칙적인 상황\n",
        "  2) 입력이 서로 다른 빈도로 샘플링된 상황\n",
        "   \n",
        "+ Flatten -> Dropout-> Dense -> Softmax -> 분류"
      ],
      "metadata": {
        "id": "dJlwyjvuuMP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using CNN for FMnist\n",
        "* Sure, here's an implementation of a convolutional neural network (CNN) that meets your requirements:\n",
        "\n",
        "In this implementation, we're using a CNN with two convolutional layers, each followed by max pooling, and a dense layer with dropout for classification. We're also using the ModelCheckpoint callback to save the best weights based on validation loss, EarlyStopping to stop training if the validation loss stops improving, and ReduceLROnPlateau to reduce the learning rate if the validation loss plateaus.\n",
        "\n",
        "After training the model for 50 epochs, the final test accuracy is printed to the console. With these settings, we can achieve a validation loss under 0.32 and a test accuracy of around 0.91. However, keep in mind that the performance of the model may vary depending on the specific hardware and software setup you're using."
      ],
      "metadata": {
        "id": "LQCSXq5zreqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape the input data for CNN\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint = ModelCheckpoint('best_weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, \n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=50, batch_size=128, verbose=1,\n",
        "                    callbacks=[checkpoint, early_stop, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqPln0gtrjjL",
        "outputId": "b870465b-cf5f-42d4-bc66-5a632ad1d1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 77s 161ms/step - loss: 0.5518 - accuracy: 0.8015 - val_loss: 0.3550 - val_accuracy: 0.8714 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 82s 175ms/step - loss: 0.3387 - accuracy: 0.8791 - val_loss: 0.3034 - val_accuracy: 0.8905 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 80s 170ms/step - loss: 0.2915 - accuracy: 0.8953 - val_loss: 0.2820 - val_accuracy: 0.8982 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 90s 191ms/step - loss: 0.2595 - accuracy: 0.9068 - val_loss: 0.2701 - val_accuracy: 0.9014 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 71s 151ms/step - loss: 0.2380 - accuracy: 0.9139 - val_loss: 0.2477 - val_accuracy: 0.9089 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 73s 156ms/step - loss: 0.2183 - accuracy: 0.9194 - val_loss: 0.2367 - val_accuracy: 0.9133 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 75s 159ms/step - loss: 0.2035 - accuracy: 0.9249 - val_loss: 0.2423 - val_accuracy: 0.9139 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 70s 150ms/step - loss: 0.1905 - accuracy: 0.9292 - val_loss: 0.2258 - val_accuracy: 0.9166 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 87s 184ms/step - loss: 0.1777 - accuracy: 0.9333 - val_loss: 0.2277 - val_accuracy: 0.9166 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 81s 173ms/step - loss: 0.1637 - accuracy: 0.9394 - val_loss: 0.2163 - val_accuracy: 0.9235 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 80s 171ms/step - loss: 0.1516 - accuracy: 0.9435 - val_loss: 0.2222 - val_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 88s 189ms/step - loss: 0.1432 - accuracy: 0.9472 - val_loss: 0.2239 - val_accuracy: 0.9220 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 82s 175ms/step - loss: 0.1288 - accuracy: 0.9520 - val_loss: 0.2238 - val_accuracy: 0.9236 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 71s 151ms/step - loss: 0.1193 - accuracy: 0.9545 - val_loss: 0.2279 - val_accuracy: 0.9219 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 73s 156ms/step - loss: 0.1126 - accuracy: 0.9586 - val_loss: 0.2279 - val_accuracy: 0.9229 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 82s 176ms/step - loss: 0.0821 - accuracy: 0.9701 - val_loss: 0.2230 - val_accuracy: 0.9341 - lr: 2.0000e-04\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 85s 182ms/step - loss: 0.0740 - accuracy: 0.9735 - val_loss: 0.2313 - val_accuracy: 0.9310 - lr: 2.0000e-04\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 78s 166ms/step - loss: 0.0702 - accuracy: 0.9754 - val_loss: 0.2305 - val_accuracy: 0.9321 - lr: 2.0000e-04\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 81s 173ms/step - loss: 0.0679 - accuracy: 0.9754 - val_loss: 0.2367 - val_accuracy: 0.9310 - lr: 2.0000e-04\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 76s 162ms/step - loss: 0.0641 - accuracy: 0.9770 - val_loss: 0.2368 - val_accuracy: 0.9319 - lr: 2.0000e-04\n",
            "Test accuracy: 0.9319000244140625\n"
          ]
        }
      ]
    }
  ]
}