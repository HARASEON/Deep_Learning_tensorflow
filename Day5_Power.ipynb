{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIZ1Po+OzLPHU716hXKa2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARASEON/Deep_Learning_tensorflow/blob/main/Day5_Power.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Power\n",
        "* Data특징:\n",
        "  * 컬럼: 7개,\n",
        "  * total: 86400 ea\n",
        "  * 60일 데이터\n",
        "* 학습방식:\n",
        "  * 24개 데이터를 읽고 다음 24개 데이터를 예측한다. (window = 24, horizon =24)\n",
        "  * 바꿔보자\n",
        "* val_loss < 0.05\n"
      ],
      "metadata": {
        "id": "mOsQ7nrKhE2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LZQI2e_UjWzi",
        "outputId": "f66c0e78-63d1-4e77-f97a-4aec3f647d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu3SzOCVfVZP",
        "outputId": "4d051eed-5e39-4f9a-8aea-bf6bffa528ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.53.0)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (67.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (16.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (4.5.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.4.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Installing collected packages: keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPbpBIlJ_I4U",
        "outputId": "e7b84dea-31fb-4337-fe85-fffb57769970"
      },
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        \n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.shuffle(len(series))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    \n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    \n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    N_PAST = 24\n",
        "    N_FUTURE = 24\n",
        "    SHIFT = 1\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    valid_set = windowed_dataset(series=x_valid, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64, \n",
        "               kernel_size=5,\n",
        "               padding=\"causal\",\n",
        "               activation=\"relu\",\n",
        "               input_shape=[N_PAST, 7],\n",
        "               ),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)), \n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.5),     \n",
        "        Dense(32, activation=\"relu\"),   \n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "    \n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_mae',\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "\n",
        "    model.compile(loss='huber',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"mae\"]\n",
        "              )\n",
        "\n",
        "    model.fit(train_set, \n",
        "          validation_data=(valid_set), \n",
        "          epochs=100, \n",
        "          callbacks=[checkpoint, early_stopping], \n",
        "          )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"power-1.h5\")\n",
        "## don't copy following codes in real test\n",
        "    from google.colab import files\n",
        "    files.download('power-1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "   1349/Unknown - 97s 60ms/step - loss: 0.0079 - mae: 0.0661\n",
            "Epoch 1: val_mae improved from inf to 0.05851, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 140s 92ms/step - loss: 0.0079 - mae: 0.0661 - val_loss: 0.0065 - val_mae: 0.0585\n",
            "Epoch 2/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0568\n",
            "Epoch 2: val_mae improved from 0.05851 to 0.05296, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 117s 82ms/step - loss: 0.0067 - mae: 0.0568 - val_loss: 0.0062 - val_mae: 0.0530\n",
            "Epoch 3/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0064 - mae: 0.0546\n",
            "Epoch 3: val_mae did not improve from 0.05296\n",
            "1349/1349 [==============================] - 98s 67ms/step - loss: 0.0064 - mae: 0.0546 - val_loss: 0.0064 - val_mae: 0.0540\n",
            "Epoch 4/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0530\n",
            "Epoch 4: val_mae improved from 0.05296 to 0.04887, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 96s 66ms/step - loss: 0.0062 - mae: 0.0530 - val_loss: 0.0059 - val_mae: 0.0489\n",
            "Epoch 5/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0512\n",
            "Epoch 5: val_mae did not improve from 0.04887\n",
            "1349/1349 [==============================] - 113s 78ms/step - loss: 0.0059 - mae: 0.0512 - val_loss: 0.0059 - val_mae: 0.0510\n",
            "Epoch 6/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0506\n",
            "Epoch 6: val_mae did not improve from 0.04887\n",
            "1349/1349 [==============================] - 113s 78ms/step - loss: 0.0058 - mae: 0.0506 - val_loss: 0.0060 - val_mae: 0.0523\n",
            "Epoch 7/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0497\n",
            "Epoch 7: val_mae improved from 0.04887 to 0.04872, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 114s 79ms/step - loss: 0.0057 - mae: 0.0497 - val_loss: 0.0059 - val_mae: 0.0487\n",
            "Epoch 8/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0495\n",
            "Epoch 8: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 117s 81ms/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.0060 - val_mae: 0.0515\n",
            "Epoch 9/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0483\n",
            "Epoch 9: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 111s 77ms/step - loss: 0.0055 - mae: 0.0483 - val_loss: 0.0059 - val_mae: 0.0505\n",
            "Epoch 10/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0484\n",
            "Epoch 10: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 99s 68ms/step - loss: 0.0055 - mae: 0.0484 - val_loss: 0.0060 - val_mae: 0.0543\n",
            "Epoch 11/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0490\n",
            "Epoch 11: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 120s 84ms/step - loss: 0.0056 - mae: 0.0490 - val_loss: 0.0060 - val_mae: 0.0503\n",
            "Epoch 12/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0473\n",
            "Epoch 12: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 113s 79ms/step - loss: 0.0053 - mae: 0.0473 - val_loss: 0.0059 - val_mae: 0.0500\n",
            "Epoch 13/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0467\n",
            "Epoch 13: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 99s 68ms/step - loss: 0.0052 - mae: 0.0467 - val_loss: 0.0061 - val_mae: 0.0520\n",
            "Epoch 14/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0463\n",
            "Epoch 14: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 114s 79ms/step - loss: 0.0051 - mae: 0.0463 - val_loss: 0.0062 - val_mae: 0.0512\n",
            "Epoch 15/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0457\n",
            "Epoch 15: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 99s 69ms/step - loss: 0.0050 - mae: 0.0457 - val_loss: 0.0062 - val_mae: 0.0530\n",
            "Epoch 16/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0454\n",
            "Epoch 16: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 97s 66ms/step - loss: 0.0049 - mae: 0.0454 - val_loss: 0.0063 - val_mae: 0.0529\n",
            "Epoch 17/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0452\n",
            "Epoch 17: val_mae did not improve from 0.04872\n",
            "1349/1349 [==============================] - 115s 80ms/step - loss: 0.0049 - mae: 0.0452 - val_loss: 0.0062 - val_mae: 0.0524\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf60b7e8-df32-4dcc-a1c1-7fb1c9d3e504\", \"power-1.h5\", 792144)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./tboard"
      ],
      "metadata": {
        "id": "1ubZDpcPlgoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "\n",
        "* Prompt: code RNN  and transfer using tensorflow  learning with  url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'  use modelcheckpoint and  early stopping and save the best model  as 'power.h5'. Modify the code to make its val_loss under 0.05.\n",
        "  \n",
        "* Respond: To improve the model's performance, here are some modifications that can be made:\n",
        "\n",
        "1. Increase the number of filters in the Conv1D layer to capture more complex features.\n",
        "2. Reduce the number of LSTM units in each layer to prevent overfitting.\n",
        "3. Add BatchNormalization after each layer to stabilize the learning process.\n",
        "4. Use the Adam optimizer with a learning rate scheduler to improve convergence speed.\n",
        "5. Increase the number of epochs to allow the model to learn more from the data.\n",
        "Here's the modified code below.\n",
        "\n",
        "* 코드 설명:\n",
        "1. Data downloading and preprocessing: The code downloads a zip file containing a CSV file with household power consumption data, which is then read and preprocessed into a pandas dataframe. The data is split into training and validation sets and normalized to have values between 0 and 1.\n",
        "\n",
        "2. Windowed dataset creation: The code defines a function to create windowed datasets from time-series data. The function takes in a series of data, the batch size, the number of past and future time steps to include in each window, and the shift between consecutive windows. The function returns a TensorFlow dataset with the appropriate windowed data format for use in training a machine learning model.\n",
        "\n",
        "3. Machine learning model: The code defines a machine learning model using the TensorFlow Keras API. The model includes several layers of convolutional and recurrent neural network layers, as well as batch normalization and dropout layers. The model is compiled with the mean absolute error loss function and the Adam optimizer.\n",
        "\n",
        "4. Model training: The code trains the machine learning model on the training dataset, with early stopping and model checkpointing to monitor progress and save the best weights. The model is also trained with a learning rate scheduler that decreases the learning rate after a certain number of epochs.\n",
        "\n",
        "5. Model saving: The code saves the trained model as a h5 file and downloads it to the local machine. This step is not necessary for running the code but is included for convenience in a Colab environment."
      ],
      "metadata": {
        "id": "Nd6Rxv5clv_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        \n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.shuffle(len(series))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    \n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    \n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    N_PAST = 24\n",
        "    N_FUTURE = 24\n",
        "    SHIFT = 1\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    valid_set = windowed_dataset(series=x_valid, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=128, \n",
        "              kernel_size=5,\n",
        "              padding=\"causal\",\n",
        "              activation=\"relu\",\n",
        "              input_shape=[N_PAST, N_FEATURES],\n",
        "              ),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(16, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(16, return_sequences=True)), \n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dropout(0.5),     \n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch < 10:\n",
        "            return lr\n",
        "        else:\n",
        "            return lr * tf.math.exp(-0.1)\n",
        "\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 save_best_only=True,\n",
        "                                 monitor='val_mae',\n",
        "                                 verbose=1,\n",
        ")\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "\n",
        "    model.compile(loss='huber',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"mae\"]\n",
        "              )\n",
        "\n",
        "    model.fit(train_set, \n",
        "          validation_data=(valid_set), \n",
        "          epochs=100, \n",
        "          callbacks=[checkpoint, early_stopping], \n",
        "          )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"power-1.h5\")\n",
        "## don't copy following codes in real test\n",
        "    from google.colab import files\n",
        "    files.download('power-1.h5')                                \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "DiqJOhcPXfzA",
        "outputId": "44573679-4350-4e31-de30-de127524c0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "   1348/Unknown - 83s 47ms/step - loss: 0.0286 - mae: 0.1551\n",
            "Epoch 1: val_mae improved from inf to 0.13409, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 125s 79ms/step - loss: 0.0286 - mae: 0.1551 - val_loss: 0.0225 - val_mae: 0.1341\n",
            "Epoch 2/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1389\n",
            "Epoch 2: val_mae did not improve from 0.13409\n",
            "1349/1349 [==============================] - 112s 78ms/step - loss: 0.0220 - mae: 0.1389 - val_loss: 0.0220 - val_mae: 0.1382\n",
            "Epoch 3/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.1374\n",
            "Epoch 3: val_mae improved from 0.13409 to 0.12875, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 90s 61ms/step - loss: 0.0217 - mae: 0.1374 - val_loss: 0.0222 - val_mae: 0.1287\n",
            "Epoch 4/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0216 - mae: 0.1367\n",
            "Epoch 4: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 109s 76ms/step - loss: 0.0216 - mae: 0.1367 - val_loss: 0.0218 - val_mae: 0.1338\n",
            "Epoch 5/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0215 - mae: 0.1362\n",
            "Epoch 5: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 109s 75ms/step - loss: 0.0215 - mae: 0.1362 - val_loss: 0.0219 - val_mae: 0.1302\n",
            "Epoch 6/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.1360\n",
            "Epoch 6: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 112s 78ms/step - loss: 0.0214 - mae: 0.1360 - val_loss: 0.0222 - val_mae: 0.1365\n",
            "Epoch 7/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0214 - mae: 0.1357\n",
            "Epoch 7: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 90s 62ms/step - loss: 0.0214 - mae: 0.1357 - val_loss: 0.0218 - val_mae: 0.1339\n",
            "Epoch 8/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0213 - mae: 0.1354\n",
            "Epoch 8: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 109s 76ms/step - loss: 0.0213 - mae: 0.1354 - val_loss: 0.0219 - val_mae: 0.1357\n",
            "Epoch 9/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.1352\n",
            "Epoch 9: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 113s 78ms/step - loss: 0.0213 - mae: 0.1352 - val_loss: 0.0221 - val_mae: 0.1332\n",
            "Epoch 10/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0212 - mae: 0.1351\n",
            "Epoch 10: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 111s 77ms/step - loss: 0.0212 - mae: 0.1351 - val_loss: 0.0219 - val_mae: 0.1373\n",
            "Epoch 11/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0212 - mae: 0.1348\n",
            "Epoch 11: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 91s 62ms/step - loss: 0.0212 - mae: 0.1348 - val_loss: 0.0218 - val_mae: 0.1332\n",
            "Epoch 12/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0212 - mae: 0.1347\n",
            "Epoch 12: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 110s 76ms/step - loss: 0.0212 - mae: 0.1347 - val_loss: 0.0218 - val_mae: 0.1348\n",
            "Epoch 13/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0211 - mae: 0.1345\n",
            "Epoch 13: val_mae did not improve from 0.12875\n",
            "1349/1349 [==============================] - 91s 62ms/step - loss: 0.0211 - mae: 0.1345 - val_loss: 0.0220 - val_mae: 0.1327\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_da9e0d3d-7083-4998-95e5-33ca58426b66\", \"power-1.h5\", 476864)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}